apiVersion: v1
data:
  cluster-backup.sh: |
    #!/usr/bin/env bash

    ### Created by cluster-etcd-operator. DO NOT edit.

    set -o errexit
    set -o pipefail
    set -o errtrace

    # example
    # cluster-backup.sh $path-to-snapshot

    if [[ $EUID -ne 0 ]]; then
      echo "This script must be run as root"
      exit 1
    fi

    function usage {
      echo 'Path to backup dir required: ./cluster-backup.sh [--force] <path-to-backup-dir>'
      exit 1
    }

    IS_DIRTY=""
    if [ "$1" == "--force" ]; then
      IS_DIRTY="__POSSIBLY_DIRTY__"
      shift
    fi

    # If the first argument is missing, or it is an existing file, then print usage and exit
    if [ -z "$1" ] || [ -f "$1" ]; then
      usage
    fi

    if [ ! -d "$1" ]; then
      mkdir -p "$1"
    fi

    function check_if_operator_is_progressing {
       local operator="$1"

       if [ ! -f "${KUBECONFIG}" ]; then
          echo "Valid kubeconfig is not found in kube-apiserver-certs. Exiting!"
          exit 1
       fi

       progressing=$(oc get co "${operator}" -o jsonpath='{.status.conditions[?(@.type=="Progressing")].status}') || true
       if [ "$progressing" == "" ]; then
          echo "Could not find the status of the $operator. Check if the API server is running. Pass the --force flag to skip checks."
          exit 1
       elif [ "$progressing" != "False" ]; then
          echo "Currently the $operator operator is progressing. A reliable backup requires that a rollout is not in progress.  Aborting!"
          exit 1
       fi
    }

    # backup latest static pod resources
    function backup_latest_kube_static_resources {
      local backup_tar_file="$1"
      local backup_resource_list=("kube-apiserver" "kube-controller-manager" "kube-scheduler" "etcd")
      local latest_resource_dirs=()

      for resource in "${backup_resource_list[@]}"; do
        if [ ! -f "/etc/kubernetes/manifests/${resource}-pod.yaml" ]; then
          echo "error finding manifests for the ${resource} pod. please check if it is running."
          exit 1
        fi

        local latest_resource
        latest_resource=$(grep -o -m 1 "/etc/kubernetes/static-pod-resources/${resource}-pod-[0-9]*" "/etc/kubernetes/manifests/${resource}-pod.yaml") || true

        if [ -z "${latest_resource}" ]; then
          echo "error finding static-pod-resources for the ${resource} pod. please check if it is running."
          exit 1
        fi
        if [ "${IS_DIRTY}" == "" ]; then
          check_if_operator_is_progressing "${resource}"
        fi

        echo "found latest ${resource}: ${latest_resource}"
        latest_resource_dirs+=("${latest_resource#${CONFIG_FILE_DIR}/}")
      done

      # tar latest resources with the path relative to CONFIG_FILE_DIR
      tar -cpzf "$backup_tar_file" -C "${CONFIG_FILE_DIR}" "${latest_resource_dirs[@]}"
      chmod 600 "$backup_tar_file"
    }

    function source_required_dependency {
      local src_path="$1"
      if [ ! -f "${src_path}" ]; then
        echo "required dependencies not found, please ensure this script is run on a node with a functional etcd static pod"
        exit 1
      fi
      # shellcheck disable=SC1090
      source "${src_path}"
    }

    BACKUP_DIR="$1"
    DATESTRING=$(date "+%F_%H%M%S")
    BACKUP_TAR_FILE=${BACKUP_DIR}/static_kuberesources_${DATESTRING}${IS_DIRTY}.tar.gz
    SNAPSHOT_FILE="${BACKUP_DIR}/snapshot_${DATESTRING}${IS_DIRTY}.db"

    trap 'rm -f ${BACKUP_TAR_FILE} ${SNAPSHOT_FILE}' ERR

    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd.env
    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd-common-tools

    # replacing the value of variables sourced form etcd.env to use the local node folders if the script is not running into the cluster-backup pod
    if [ ! -f "${ETCDCTL_CACERT}" ]; then
      echo "Certificate ${ETCDCTL_CACERT} is missing. Checking in different directory"
      export ETCDCTL_CACERT=$(echo ${ETCDCTL_CACERT} | sed -e "s|static-pod-certs|static-pod-resources/etcd-certs|")
      export ETCDCTL_CERT=$(echo ${ETCDCTL_CERT} | sed -e "s|static-pod-certs|static-pod-resources/etcd-certs|")
      export ETCDCTL_KEY=$(echo ${ETCDCTL_KEY} | sed -e "s|static-pod-certs|static-pod-resources/etcd-certs|")
      if [ ! -f "${ETCDCTL_CACERT}" ]; then
        echo "Certificate ${ETCDCTL_CACERT} is also missing in the second directory. Exiting!"
        exit 1
      else
        echo "Certificate ${ETCDCTL_CACERT} found!"
      fi
    fi

    backup_latest_kube_static_resources "${BACKUP_TAR_FILE}"

    # Download etcdctl and get the etcd snapshot
    dl_etcdctl
    ETCDCTL_ENDPOINTS="https://${NODE_NODE_ENVVAR_NAME_IP}:2379" etcdctl snapshot save "${SNAPSHOT_FILE}"

    # Check the integrity of the snapshot
    check_snapshot_status "${SNAPSHOT_FILE}"
    snapshot_failed=$?

    # If check_snapshot_status returned 1 it failed, so exit with code 1
    if [[ $snapshot_failed -eq 1 ]]; then
      exit 1
    fi

    echo "snapshot db and kube resources are successfully saved to ${BACKUP_DIR}"
  cluster-restore.sh: |
    #!/usr/bin/env bash

    ### Created by cluster-etcd-operator. DO NOT edit.

    set -o errexit
    set -o pipefail
    set -o errtrace

    # example
    # ./cluster-restore.sh $path-to-backup

    if [[ $EUID -ne 0 ]]; then
      echo "This script must be run as root"
      exit 1
    fi

    function source_required_dependency {
      local src_path="$1"
      if [ ! -f "${src_path}" ]; then
        echo "required dependencies not found, please ensure this script is run on a node with a functional etcd static pod"
        exit 1
      fi
      # shellcheck disable=SC1090
      source "${src_path}"
    }

    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd.env
    source_required_dependency /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-scripts/etcd-common-tools

    function usage() {
      echo 'Path to the directory containing backup files is required: ./cluster-restore.sh <path-to-backup>'
      echo 'The backup directory is expected to be contain two files:'
      echo '        1. etcd snapshot'
      echo '        2. A copy of the Static POD resources at the time of backup'
      exit 1
    }

    # If the argument is not passed, or if it is not a directory, print usage and exit.
    if [ "$1" == "" ] || [ ! -d "$1" ]; then
      usage
    fi

    function restore_static_pods() {
      local backup_file="$1"
      shift
      local static_pods=("$@")

      for pod_file_name in "${static_pods[@]}"; do
        backup_pod_path=$(tar -tvf "${backup_file}" "*${pod_file_name}" | awk '{ print $6 }') || true
        if [ -z "${backup_pod_path}" ]; then
          echo "${pod_file_name} does not exist in ${backup_file}"
          exit 1
        fi

        echo "starting ${pod_file_name}"
        tar -xvf "${backup_file}" --strip-components=2 -C "${MANIFEST_DIR}"/ "${backup_pod_path}"
      done
    }

    function wait_for_containers_to_stop() {
      local containers=("$@")

      for container_name in "${containers[@]}"; do
        echo "Waiting for container ${container_name} to stop"
        while [[ -n $(crictl ps --label io.kubernetes.container.name="${container_name}" -q) ]]; do
          echo -n "."
          sleep 1
        done
        echo "complete"
      done
    }

    BACKUP_DIR="$1"
    # shellcheck disable=SC2012
    BACKUP_FILE=$(ls -vd "${BACKUP_DIR}"/static_kuberesources*.tar.gz | tail -1) || true
    # shellcheck disable=SC2012
    SNAPSHOT_FILE=$(ls -vd "${BACKUP_DIR}"/snapshot*.db | tail -1) || true
    STATIC_POD_LIST=("kube-apiserver-pod.yaml" "kube-controller-manager-pod.yaml" "kube-scheduler-pod.yaml")
    STATIC_POD_CONTAINERS=("etcd" "etcdctl" "etcd-metrics" "kube-controller-manager" "kube-apiserver" "kube-scheduler")

    if [ ! -f "${SNAPSHOT_FILE}" ]; then
      echo "etcd snapshot ${SNAPSHOT_FILE} does not exist"
      exit 1
    fi

    # Download etcdctl and check the snapshot status
    dl_etcdctl
    check_snapshot_status "${SNAPSHOT_FILE}"

    # Move manifests and stop static pods
    if [ ! -d "$MANIFEST_STOPPED_DIR" ]; then
      mkdir -p "$MANIFEST_STOPPED_DIR"
    fi

    # Move static pod manifests out of MANIFEST_DIR
    for POD_FILE_NAME in "${STATIC_POD_LIST[@]}" etcd-pod.yaml; do
      echo "...stopping ${POD_FILE_NAME}"
      [ ! -f "${MANIFEST_DIR}/${POD_FILE_NAME}" ] && continue
      mv "${MANIFEST_DIR}/${POD_FILE_NAME}" "${MANIFEST_STOPPED_DIR}"
    done

    # wait for every static pod container to stop
    wait_for_containers_to_stop "${STATIC_POD_CONTAINERS[@]}"

    if [ ! -d "${ETCD_DATA_DIR_BACKUP}" ]; then
      mkdir -p "${ETCD_DATA_DIR_BACKUP}"
    fi

    # backup old data-dir
    if [ -d "${ETCD_DATA_DIR}/member" ]; then
      if [ -d "${ETCD_DATA_DIR_BACKUP}/member" ]; then
        echo "removing previous backup ${ETCD_DATA_DIR_BACKUP}/member"
        rm -rf "${ETCD_DATA_DIR_BACKUP}"/member
      fi
      echo "Moving etcd data-dir ${ETCD_DATA_DIR}/member to ${ETCD_DATA_DIR_BACKUP}"
      mv "${ETCD_DATA_DIR}"/member "${ETCD_DATA_DIR_BACKUP}"/
    fi

    # Restore static pod resources
    tar -C "${CONFIG_FILE_DIR}" -xzf "${BACKUP_FILE}" static-pod-resources

    # Copy snapshot to backupdir
    cp -p "${SNAPSHOT_FILE}" "${ETCD_DATA_DIR_BACKUP}"/snapshot.db

    echo "starting restore-etcd static pod"
    cp -p "${RESTORE_ETCD_POD_YAML}" "${MANIFEST_DIR}/etcd-pod.yaml"

    # start remaining static pods
    restore_static_pods "${BACKUP_FILE}" "${STATIC_POD_LIST[@]}"
  etcd-common-tools: |+
    # Common environment variables
    ASSET_DIR="/home/core/assets"
    CONFIG_FILE_DIR="/etc/kubernetes"
    MANIFEST_DIR="${CONFIG_FILE_DIR}/manifests"
    ETCD_DATA_DIR="/var/lib/etcd"
    ETCD_DATA_DIR_BACKUP="/var/lib/etcd-backup"
    MANIFEST_STOPPED_DIR="${ASSET_DIR}/manifests-stopped"
    RESTORE_ETCD_POD_YAML="${CONFIG_FILE_DIR}/static-pod-resources/etcd-certs/configmaps/restore-etcd-pod/pod.yaml"
    ETCDCTL_BIN_DIR="${CONFIG_FILE_DIR}/static-pod-resources/bin"
    PATH=${PATH}:${ETCDCTL_BIN_DIR}
    KUBECONFIG="/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig"
    export KUBECONFIG

    # download etcdctl from upstream release assets
    function dl_etcdctl {
      if [ -x "$(command -v etcdctl)" ]; then
        echo "etcdctl is already installed"
        return
      fi
      local etcdimg=${ETCD_IMAGE}
      local etcdctr=$(podman create ${etcdimg} --authfile=/var/lib/kubelet/config.json)
      local etcdmnt=$(podman mount "${etcdctr}")
      [ ! -d ${ETCDCTL_BIN_DIR} ] && mkdir -p ${ETCDCTL_BIN_DIR}
      cp ${etcdmnt}/bin/etcdctl ${ETCDCTL_BIN_DIR}/
      umount "${etcdmnt}"
      podman rm "${etcdctr}"
      etcdctl version
    }

    # execute etcdctl command inside of running etcdctl container
    function exec_etcdctl {
      local command="$@"
      local container_id=$(sudo crictl ps --label io.kubernetes.container.name=etcdctl -o json | jq -r '.containers[0].id') || true
      if [ -z "$container_id" ]; then
        echo "etcdctl container is not running"
        exit 1
      fi
      crictl exec -it $container_id /bin/sh -c "etcdctl $command"
    }

    function check_snapshot_status() {
      local snap_file="$1"
      if ! etcdctl snapshot status "${snap_file}" -w json; then
        echo "Backup integrity verification failed. Backup appears corrupted. Aborting!"
        return 1
      fi
    }

  etcd.env: |
    export ALL_ETCD_ENDPOINTS="https://10.0.130.93:2379"
    export ETCDCTL_API="3"
    export ETCDCTL_CACERT="/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt"
    export ETCDCTL_CERT="/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-NODE_NAME.crt"
    export ETCDCTL_ENDPOINTS="https://10.0.130.93:2379"
    export ETCDCTL_KEY="/etc/kubernetes/static-pod-certs/secrets/etcd-all-certs/etcd-peer-NODE_NAME.key"
    export ETCD_CIPHER_SUITES="TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256"
    export ETCD_DATA_DIR="/var/lib/etcd"
    export ETCD_ELECTION_TIMEOUT="1000"
    export ETCD_ENABLE_PPROF="true"
    export ETCD_EXPERIMENTAL_MAX_LEARNERS="1"
    export ETCD_EXPERIMENTAL_WARNING_APPLY_DURATION="200ms"
    export ETCD_EXPERIMENTAL_WATCH_PROGRESS_NOTIFY_INTERVAL="5s"
    export ETCD_HEARTBEAT_INTERVAL="100"
    export ETCD_IMAGE="registry.build05.ci.openshift.org/ci-ln-88xnydk/stable@sha256:c319ee887362ff3ce889126e7be803ab11cd331ea89c7ac43a93c33ed3bc4b76"
    export ETCD_INITIAL_CLUSTER_STATE="existing"
    export ETCD_QUOTA_BACKEND_BYTES="8589934592"
    export ETCD_SOCKET_REUSE_ADDRESS="true"
    export NODE_vrutkovs_sno_ETCD_NAME="vrutkovs-sno"
    export NODE_vrutkovs_sno_ETCD_URL_HOST="10.0.130.93"
    export NODE_vrutkovs_sno_IP="10.0.130.93"
kind: ConfigMap
metadata:
  creationTimestamp: "2023-03-09T14:22:39Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:cluster-backup.sh: {}
        f:cluster-restore.sh: {}
        f:etcd-common-tools: {}
        f:etcd.env: {}
    manager: cluster-etcd-operator
    operation: Update
    time: "2023-03-09T14:22:39Z"
  name: etcd-scripts
  namespace: openshift-etcd
  resourceVersion: "5737"
  uid: e6a5bc5e-0ed7-444d-8bf8-1f53121a0c96
