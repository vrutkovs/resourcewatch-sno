apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-03-08T12:19:55Z"
  generateName: node-resolver-
  labels:
    controller-revision-hash: 64fb48c688
    dns.operator.openshift.io/daemonset-node-resolver: ""
    pod-template-generation: "1"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:target.workload.openshift.io/management: {}
        f:generateName: {}
        f:labels:
          .: {}
          f:controller-revision-hash: {}
          f:dns.operator.openshift.io/daemonset-node-resolver: {}
          f:pod-template-generation: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"2de761ea-9d60-4d2f-86d6-ed77b20ff944"}: {}
      f:spec:
        f:affinity:
          .: {}
          f:nodeAffinity:
            .: {}
            f:requiredDuringSchedulingIgnoredDuringExecution: {}
        f:containers:
          k:{"name":"dns-node-resolver"}:
            .: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"CLUSTER_DOMAIN"}:
                .: {}
                f:name: {}
                f:value: {}
              k:{"name":"NAMESERVER"}:
                .: {}
                f:name: {}
                f:value: {}
              k:{"name":"SERVICES"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:securityContext:
              .: {}
              f:privileged: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/hosts"}:
                .: {}
                f:mountPath: {}
                f:name: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:hostNetwork: {}
        f:nodeSelector: {}
        f:priorityClassName: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:serviceAccount: {}
        f:serviceAccountName: {}
        f:terminationGracePeriodSeconds: {}
        f:tolerations: {}
        f:volumes:
          .: {}
          k:{"name":"hosts-file"}:
            .: {}
            f:hostPath:
              .: {}
              f:path: {}
              f:type: {}
            f:name: {}
    manager: kube-controller-manager
    operation: Update
    time: "2023-03-08T12:19:55Z"
  name: node-resolver-xk5zd
  namespace: openshift-dns
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: DaemonSet
    name: node-resolver
    uid: 2de761ea-9d60-4d2f-86d6-ed77b20ff944
  resourceVersion: "11915"
  uid: 2dd07a2a-7025-48ea-8c5c-838bc0f77fc2
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: In
            values:
            - vrutkovs-sno
  containers:
  - command:
    - /bin/bash
    - -c
    - |
      #!/bin/bash
      set -uo pipefail

      trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

      OPENSHIFT_MARKER="openshift-generated-node-resolver"
      HOSTS_FILE="/etc/hosts"
      TEMP_FILE="/etc/hosts.tmp"

      IFS=', ' read -r -a services <<< "${SERVICES}"

      # Make a temporary file with the old hosts file's attributes.
      if ! cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"; then
        echo "Failed to preserve hosts file. Exiting."
        exit 1
      fi

      while true; do
        declare -A svc_ips
        for svc in "${services[@]}"; do
          # Fetch service IP from cluster dns if present. We make several tries
          # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
          # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
          # support UDP loadbalancers and require reaching DNS through TCP.
          cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
          for i in ${!cmds[*]}
          do
            ips=($(eval "${cmds[i]}"))
            if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
              svc_ips["${svc}"]="${ips[@]}"
              break
            fi
          done
        done

        # Update /etc/hosts only if we get valid service IPs
        # We will not update /etc/hosts when there is coredns service outage or api unavailability
        # Stale entries could exist in /etc/hosts if the service is deleted
        if [[ -n "${svc_ips[*]-}" ]]; then
          # Build a new hosts file from /etc/hosts with our custom entries filtered out
          if ! sed --silent "/# ${OPENSHIFT_MARKER}/d; w ${TEMP_FILE}" "${HOSTS_FILE}"; then
            # Only continue rebuilding the hosts entries if its original content is preserved
            sleep 60 & wait
            continue
          fi

          # Append resolver entries for services
          for svc in "${!svc_ips[@]}"; do
            for ip in ${svc_ips[${svc}]}; do
              echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
            done
          done

          # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
          # Replace /etc/hosts with our modified version if needed
          cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
          # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
        fi
        sleep 60 & wait
        unset svc_ips
      done
    env:
    - name: SERVICES
      value: image-registry.openshift-image-registry.svc
    - name: NAMESERVER
      value: 172.30.0.10
    - name: CLUSTER_DOMAIN
      value: cluster.local
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9af4bd001d30fd00d89c5f199b970b820240d0959312c2fb8ea82597c8da24bb
    imagePullPolicy: IfNotPresent
    name: dns-node-resolver
    resources:
      requests:
        cpu: 5m
        memory: 21Mi
    securityContext:
      privileged: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/hosts
      name: hosts-file
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-4t5k6
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostNetwork: true
  imagePullSecrets:
  - name: node-resolver-dockercfg-66bcj
  nodeSelector:
    kubernetes.io/os: linux
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: node-resolver
  serviceAccountName: node-resolver
  terminationGracePeriodSeconds: 30
  tolerations:
  - operator: Exists
  volumes:
  - hostPath:
      path: /etc/hosts
      type: File
    name: hosts-file
  - name: kube-api-access-4t5k6
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  phase: Pending
  qosClass: Burstable
