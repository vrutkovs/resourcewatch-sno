apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/default-container: kube-controller-manager
    kubernetes.io/config.hash: a6653c6a4111ccb269ba9b2346e0260d
    kubernetes.io/config.mirror: a6653c6a4111ccb269ba9b2346e0260d
    kubernetes.io/config.seen: "2023-03-08T12:16:21.168841726Z"
    kubernetes.io/config.source: file
    target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
  creationTimestamp: "2023-03-08T12:17:03Z"
  labels:
    app: kube-controller-manager
    kube-controller-manager: "true"
    revision: "4"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/default-container: {}
          f:kubernetes.io/config.hash: {}
          f:kubernetes.io/config.mirror: {}
          f:kubernetes.io/config.seen: {}
          f:kubernetes.io/config.source: {}
          f:target.workload.openshift.io/management: {}
        f:labels:
          .: {}
          f:app: {}
          f:kube-controller-manager: {}
          f:revision: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"7ba64a87-d15f-47ca-ae90-36dfe6a9c47e"}: {}
      f:spec:
        f:containers:
          k:{"name":"cluster-policy-controller"}:
            .: {}
            f:args: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"POD_NAME"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
              k:{"name":"POD_NAMESPACE"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:livenessProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:initialDelaySeconds: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":10357,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:hostPort: {}
                f:protocol: {}
            f:readinessProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:initialDelaySeconds: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:startupProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-certs"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-resources"}:
                .: {}
                f:mountPath: {}
                f:name: {}
          k:{"name":"kube-controller-manager"}:
            .: {}
            f:args: {}
            f:command: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:livenessProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:initialDelaySeconds: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":10257,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:hostPort: {}
                f:protocol: {}
            f:readinessProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:initialDelaySeconds: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:startupProbe:
              .: {}
              f:failureThreshold: {}
              f:httpGet:
                .: {}
                f:path: {}
                f:port: {}
                f:scheme: {}
              f:periodSeconds: {}
              f:successThreshold: {}
              f:timeoutSeconds: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-certs"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-resources"}:
                .: {}
                f:mountPath: {}
                f:name: {}
          k:{"name":"kube-controller-manager-cert-syncer"}:
            .: {}
            f:args: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"POD_NAME"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
              k:{"name":"POD_NAMESPACE"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-certs"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-resources"}:
                .: {}
                f:mountPath: {}
                f:name: {}
          k:{"name":"kube-controller-manager-recovery-controller"}:
            .: {}
            f:args: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"POD_NAMESPACE"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-certs"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/kubernetes/static-pod-resources"}:
                .: {}
                f:mountPath: {}
                f:name: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:hostNetwork: {}
        f:nodeName: {}
        f:priorityClassName: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:terminationGracePeriodSeconds: {}
        f:tolerations: {}
        f:volumes:
          .: {}
          k:{"name":"cert-dir"}:
            .: {}
            f:hostPath:
              .: {}
              f:path: {}
              f:type: {}
            f:name: {}
          k:{"name":"resource-dir"}:
            .: {}
            f:hostPath:
              .: {}
              f:path: {}
              f:type: {}
            f:name: {}
    manager: kubelet
    operation: Update
    time: "2023-03-08T12:17:03Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          .: {}
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:message: {}
            f:reason: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"PodScheduled"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:message: {}
            f:reason: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.0.130.68"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    subresource: status
    time: "2023-03-08T12:21:06Z"
  name: kube-controller-manager-vrutkovs-sno
  namespace: openshift-kube-controller-manager
  ownerReferences:
  - apiVersion: v1
    controller: true
    kind: Node
    name: vrutkovs-sno
    uid: 7ba64a87-d15f-47ca-ae90-36dfe6a9c47e
  resourceVersion: "13038"
  uid: 1855f52b-8103-4cb1-a237-f58334c73140
spec:
  containers:
  - args:
    - |-
      timeout 3m /bin/bash -exuo pipefail -c 'while [ -n "$(ss -Htanop \( sport = 10257 \))" ]; do sleep 1; done'

      if [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then
        echo "Copying system trust bundle"
        cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi

      if [ -f /etc/kubernetes/static-pod-resources/configmaps/cloud-config/ca-bundle.pem ]; then
        echo "Setting custom CA bundle for cloud provider"
        export AWS_CA_BUNDLE=/etc/kubernetes/static-pod-resources/configmaps/cloud-config/ca-bundle.pem
      fi

      exec hyperkube kube-controller-manager --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml \
        --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --authentication-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --authorization-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --client-ca-file=/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt \
        --requestheader-client-ca-file=/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt -v=2 --allocate-node-cidrs=false --cert-dir=/var/run/kubernetes --cluster-cidr=10.128.0.0/14 --cluster-name=vrutkovs-q974t --cluster-signing-cert-file=/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt --cluster-signing-duration=720h --cluster-signing-key-file=/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key --configure-cloud-routes=false --controllers=* --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-ttl --enable-dynamic-provisioning=true --feature-gates=APIPriorityAndFairness=true --feature-gates=DownwardAPIHugePages=true --feature-gates=OpenShiftPodSecurityAdmission=true --feature-gates=RetroactiveDefaultStorageClass=false --feature-gates=RotateKubeletServerCertificate=true --flex-volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --kube-api-burst=300 --kube-api-qps=150 --leader-elect-renew-deadline=12s --leader-elect-resource-lock=configmapsleases --leader-elect-retry-period=3s --leader-elect=true --pv-recycler-pod-template-filepath-hostpath=/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml --pv-recycler-pod-template-filepath-nfs=/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml --root-ca-file=/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt --secure-port=10257 --service-account-private-key-file=/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key --service-cluster-ip-range=172.30.0.0/16 --use-service-account-credentials=true --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 --tls-min-version=VersionTLS12
    command:
    - /bin/bash
    - -euxo
    - pipefail
    - -c
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f88cd82b162343cd0e3455c1d39d0b2835e6a0e2ba99e640c12df2661d5595fd
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 45
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    name: kube-controller-manager
    ports:
    - containerPort: 10257
      hostPort: 10257
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    resources:
      requests:
        cpu: 60m
        memory: 200Mi
    startupProbe:
      failureThreshold: 3
      httpGet:
        path: healthz
        port: 10257
        scheme: HTTPS
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 3
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/kubernetes/static-pod-resources
      name: resource-dir
    - mountPath: /etc/kubernetes/static-pod-certs
      name: cert-dir
  - args:
    - |-
      timeout 3m /bin/bash -exuo pipefail -c 'while [ -n "$(ss -Htanop \( sport = 10357 \))" ]; do sleep 1; done'

      exec cluster-policy-controller start --config=/etc/kubernetes/static-pod-resources/configmaps/cluster-policy-controller-config/config.yaml \
        --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --namespace=${POD_NAMESPACE} -v=2
    command:
    - /bin/bash
    - -euxo
    - pipefail
    - -c
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7b3d70b1257fa3291a5b364dec35289e3c7df523d69241c9bd01158d1f2141ac
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: healthz
        port: 10357
        scheme: HTTPS
      initialDelaySeconds: 45
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    name: cluster-policy-controller
    ports:
    - containerPort: 10357
      hostPort: 10357
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: healthz
        port: 10357
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    resources:
      requests:
        cpu: 10m
        memory: 200Mi
    startupProbe:
      failureThreshold: 3
      httpGet:
        path: healthz
        port: 10357
        scheme: HTTPS
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 3
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/kubernetes/static-pod-resources
      name: resource-dir
    - mountPath: /etc/kubernetes/static-pod-certs
      name: cert-dir
  - args:
    - --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-controller-cert-syncer-kubeconfig/kubeconfig
    - --namespace=$(POD_NAMESPACE)
    - --destination-dir=/etc/kubernetes/static-pod-certs
    command:
    - cluster-kube-controller-manager-operator
    - cert-syncer
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e3c201c17dba67a00ba69b15c1bc1ed24a408c239746df507c78c8eb3a691f5f
    imagePullPolicy: IfNotPresent
    name: kube-controller-manager-cert-syncer
    resources:
      requests:
        cpu: 5m
        memory: 50Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/kubernetes/static-pod-resources
      name: resource-dir
    - mountPath: /etc/kubernetes/static-pod-certs
      name: cert-dir
  - args:
    - |-
      timeout 3m /bin/bash -exuo pipefail -c 'while [ -n "$(ss -Htanop \( sport = 9443 \))" ]; do sleep 1; done'

      exec cluster-kube-controller-manager-operator cert-recovery-controller --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-controller-cert-syncer-kubeconfig/kubeconfig --namespace=${POD_NAMESPACE} --listen=0.0.0.0:9443 -v=2
    command:
    - /bin/bash
    - -euxo
    - pipefail
    - -c
    env:
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e3c201c17dba67a00ba69b15c1bc1ed24a408c239746df507c78c8eb3a691f5f
    imagePullPolicy: IfNotPresent
    name: kube-controller-manager-recovery-controller
    resources:
      requests:
        cpu: 5m
        memory: 50Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/kubernetes/static-pod-resources
      name: resource-dir
    - mountPath: /etc/kubernetes/static-pod-certs
      name: cert-dir
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  hostNetwork: true
  nodeName: vrutkovs-sno
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  terminationGracePeriodSeconds: 30
  tolerations:
  - operator: Exists
  volumes:
  - hostPath:
      path: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4
      type: ""
    name: resource-dir
  - hostPath:
      path: /etc/kubernetes/static-pod-resources/kube-controller-manager-certs
      type: ""
    name: cert-dir
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:15:28Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:21:06Z"
    message: 'containers with unready status: [kube-controller-manager cluster-policy-controller
      kube-controller-manager-cert-syncer kube-controller-manager-recovery-controller]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:21:06Z"
    message: 'containers with unready status: [kube-controller-manager cluster-policy-controller
      kube-controller-manager-cert-syncer kube-controller-manager-recovery-controller]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:15:28Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://b76e624f1a8a9d92407ac41052bcec9bdaada5d8aaff217d6c2851c6b33a4a93
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7b3d70b1257fa3291a5b364dec35289e3c7df523d69241c9bd01158d1f2141ac
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7b3d70b1257fa3291a5b364dec35289e3c7df523d69241c9bd01158d1f2141ac
    lastState:
      terminated:
        containerID: cri-o://69d55dee94f71f75e41b67e6f099a5fd4308cce81c3b9cc9d112297cceecabca
        exitCode: 137
        finishedAt: "2023-03-08T12:17:34Z"
        message: "hes are synced for namespace-security-allocation-controller \nI0308
          12:17:23.705841       1 base_controller.go:110] Starting #1 worker of namespace-security-allocation-controller
          controller ...\nI0308 12:17:23.705913       1 namespace_scc_allocation_controller.go:111]
          Repairing SCC UID Allocations\nW0308 12:17:23.942525       1 reflector.go:424]
          k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream:
          the server could not find the requested resource (get imagestreams.image.openshift.io)\nE0308
          12:17:23.942560       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169:
          Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server
          could not find the requested resource (get imagestreams.image.openshift.io)\nI0308
          12:17:24.802584       1 request.go:601] Waited for 2.396787755s due to client-side
          throttling, not priority and fairness, request: GET:https://api-int.vrutkovs.choomba.one:6443/api/v1/limitranges?limit=500&resourceVersion=0\nI0308
          12:17:25.232504       1 namespace_scc_allocation_controller.go:116] Repair
          complete\nW0308 12:17:25.674723       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169:
          failed to list *v1.ImageStream: the server could not find the requested
          resource (get imagestreams.image.openshift.io)\nE0308 12:17:25.674749       1
          reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169:
          Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server
          could not find the requested resource (get imagestreams.image.openshift.io)\nW0308
          12:17:29.578876       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169:
          failed to list *v1.ImageStream: the server could not find the requested
          resource (get imagestreams.image.openshift.io)\nE0308 12:17:29.578902       1
          reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169:
          Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server
          could not find the requested resource (get imagestreams.image.openshift.io)\n"
        reason: Error
        startedAt: "2023-03-08T12:16:34Z"
    name: cluster-policy-controller
    ready: false
    restartCount: 1
    started: false
    state:
      terminated:
        containerID: cri-o://b76e624f1a8a9d92407ac41052bcec9bdaada5d8aaff217d6c2851c6b33a4a93
        exitCode: 0
        finishedAt: "2023-03-08T12:21:06Z"
        reason: Completed
        startedAt: "2023-03-08T12:17:34Z"
  - containerID: cri-o://da430329c5e554c4d7deefba86d6fa52717e7880b29ad3419c2c543741454687
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f88cd82b162343cd0e3455c1d39d0b2835e6a0e2ba99e640c12df2661d5595fd
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f88cd82b162343cd0e3455c1d39d0b2835e6a0e2ba99e640c12df2661d5595fd
    lastState:
      terminated:
        containerID: cri-o://0a952b5fae22cafb86f72827fcf450f5054a564120e785f765550112e46cfb51
        exitCode: 1
        finishedAt: "2023-03-08T12:19:30Z"
        message: |
          tc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt::/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key"
          I0308 12:19:30.774560       1 pv_controller_base.go:602] claim worker queue shutting down
          I0308 12:19:30.774179       1 cronjob_controllerv2.go:149] "Shutting down cronjob controller v2"
          I0308 12:19:30.774585       1 resource_quota_monitor.go:329] QuotaMonitor stopped 73 of 73 monitors
          I0308 12:19:30.774589       1 resource_quota_monitor.go:330] QuotaMonitor stopping
          I0308 12:19:30.774604       1 graph_builder.go:319] stopped 159 of 159 monitors
          I0308 12:19:30.774281       1 certificate_controller.go:124] Shutting down certificate controller "csrsigning-kubelet-client"
          I0308 12:19:30.774612       1 pv_controller_base.go:545] volume worker queue shutting down
          I0308 12:19:30.774299       1 certificate_controller.go:124] Shutting down certificate controller "csrsigning-kube-apiserver-client"
          I0308 12:19:30.774611       1 graph_builder.go:320] GraphBuilder stopping
          I0308 12:19:30.774460       1 resource_quota_controller.go:264] resource quota controller worker shutting down
          I0308 12:19:30.774636       1 dynamic_serving_content.go:146] "Shutting down controller" name="csr-controller::/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt::/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key"
          I0308 12:19:30.774650       1 horizontal.go:242] horizontal pod autoscaler controller worker shutting down
          I0308 12:19:30.774658       1 horizontal.go:242] horizontal pod autoscaler controller worker shutting down
          I0308 12:19:30.774662       1 horizontal.go:242] horizontal pod autoscaler controller worker shutting down
          I0308 12:19:30.774665       1 horizontal.go:242] horizontal pod autoscaler controller worker shutting down
          I0308 12:19:30.774106       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager" fieldPath="" kind="Lease" apiVersion="coordination.k8s.io/v1" type="Normal" reason="LeaderElection" message="vrutkovs-sno_a012d12f-1697-4b1c-91c7-23bce894d8d1 stopped leading"
        reason: Error
        startedAt: "2023-03-08T12:16:34Z"
    name: kube-controller-manager
    ready: false
    restartCount: 1
    started: false
    state:
      terminated:
        containerID: cri-o://da430329c5e554c4d7deefba86d6fa52717e7880b29ad3419c2c543741454687
        exitCode: 0
        finishedAt: "2023-03-08T12:21:06Z"
        reason: Completed
        startedAt: "2023-03-08T12:19:31Z"
  - containerID: cri-o://6c865148bcc4def8e2d3a79713bd7be0f9c85895b3bc7a781ea7d3a6adeb312d
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e3c201c17dba67a00ba69b15c1bc1ed24a408c239746df507c78c8eb3a691f5f
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e3c201c17dba67a00ba69b15c1bc1ed24a408c239746df507c78c8eb3a691f5f
    lastState: {}
    name: kube-controller-manager-cert-syncer
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: cri-o://6c865148bcc4def8e2d3a79713bd7be0f9c85895b3bc7a781ea7d3a6adeb312d
        exitCode: 2
        finishedAt: "2023-03-08T12:21:06Z"
        message: |
          3223       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:20:07.953471       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
          I0308 12:20:08.552806       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:20:08.553043       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
          I0308 12:20:14.714072       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:20:14.714379       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
          I0308 12:20:34.004138       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:20:34.004451       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
          I0308 12:20:34.016566       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:20:34.016827       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
          I0308 12:20:40.728581       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:20:40.728828       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
          I0308 12:21:00.030004       1 certsync_controller.go:66] Syncing configmaps: [{aggregator-client-ca false} {client-ca false} {trusted-ca-bundle true}]
          I0308 12:21:00.030246       1 certsync_controller.go:170] Syncing secrets: [{kube-controller-manager-client-cert-key false} {csr-signer false}]
        reason: Error
        startedAt: "2023-03-08T12:16:34Z"
  - containerID: cri-o://0dfed3f15581ca2678c4a26846613a901fbbc5a0755cafa98c81d94e71e45437
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e3c201c17dba67a00ba69b15c1bc1ed24a408c239746df507c78c8eb3a691f5f
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e3c201c17dba67a00ba69b15c1bc1ed24a408c239746df507c78c8eb3a691f5f
    lastState: {}
    name: kube-controller-manager-recovery-controller
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: cri-o://0dfed3f15581ca2678c4a26846613a901fbbc5a0755cafa98c81d94e71e45437
        exitCode: 0
        finishedAt: "2023-03-08T12:21:06Z"
        reason: Completed
        startedAt: "2023-03-08T12:16:34Z"
  hostIP: 10.0.130.68
  phase: Running
  podIP: 10.0.130.68
  podIPs:
  - ip: 10.0.130.68
  qosClass: Burstable
  startTime: "2023-03-08T12:15:28Z"
