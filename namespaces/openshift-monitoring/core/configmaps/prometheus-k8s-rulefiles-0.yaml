apiVersion: v1
data:
  openshift-cloud-credential-operator-cloud-credential-operator-alerts-8bfd7282-36ce-4525-8b44-8dff286ebc01.yaml: |
    groups:
    - name: CloudCredentialOperator
      rules:
      - alert: CloudCredentialOperatorTargetNamespaceMissing
        annotations:
          description: At least one CredentialsRequest custom resource has specified in
            its .spec.secretRef.namespace field a namespace which does not presently exist.
            This means the Cloud Credential Operator in the openshift-cloud-credential-operator
            namespace cannot process the CredentialsRequest resource. Check the conditions
            of all CredentialsRequests with 'oc get credentialsrequest -A' to find any
            CredentialsRequest(s) with a .status.condition showing a condition type of
            MissingTargetNamespace set to True.
          message: CredentialsRequest(s) pointing to non-existent namespace
          summary: One ore more CredentialsRequest CRs are asking to save credentials
            to a non-existent namespace.
        expr: cco_credentials_requests_conditions{condition="MissingTargetNamespace"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorProvisioningFailed
        annotations:
          description: While processing a CredentialsRequest, the Cloud Credential Operator
            encountered an issue. Check the conditions of all CredentialsRequets with
            'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .stats.condition
            showing a condition type of CredentialsProvisionFailure set to True for more
            details on the issue.
          message: CredentialsRequest(s) unable to be fulfilled
          summary: One or more CredentialsRequest CRs are unable to be processed.
        expr: cco_credentials_requests_conditions{condition="CredentialsProvisionFailure"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorDeprovisioningFailed
        annotations:
          description: While processing a CredentialsRequest marked for deletion, the
            Cloud Credential Operator encountered an issue. Check the conditions of all
            CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s)
            with a .status.condition showing a condition type of CredentialsDeprovisionFailure
            set to True for more details on the issue.
          message: CredentialsRequest(s) unable to be cleaned up
          summary: One or more CredentialsRequest CRs are unable to be deleted.
        expr: cco_credentials_requests_conditions{condition="CredentialsDeprovisionFailure"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorInsufficientCloudCreds
        annotations:
          description: The Cloud Credential Operator has determined that there are insufficient
            permissions to process one or more CredentialsRequest CRs. Check the conditions
            of all CredentialsRequests with 'oc get credentialsrequest -A' to find any
            CredentialsRequest(s) with a .status.condition showing a condition type of
            InsufficientCloudCreds set to True for more details.
          message: Cluster's cloud credentials insufficient for minting or passthrough
          summary: Problem with the available platform credentials.
        expr: cco_credentials_requests_conditions{condition="InsufficientCloudCreds"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorStaleCredentials
        annotations:
          description: The Cloud Credential Operator (CCO) has detected one or more stale
            CredentialsRequest CRs that need to be manually deleted. When the CCO is in
            Manual credentials mode, it will not automatially clean up stale CredentialsRequest
            CRs (that may no longer be necessary in the present version of OpenShift because
            it could involve needing to clean up manually created cloud resources. Check
            the conditions of all CredentialsRequests with 'oc get credentialsrequest
            -A' to find any CredentialsRequest(s) with a .status.condition showing a condition
            type of StaleCredentials set to True. Determine the appropriate steps to clean
            up/deprovision any previously provisioned cloud resources. Finally, delete
            the CredentialsRequest with an 'oc delete'.
          message: 1 or more credentials requests are stale and should be deleted. Check
            the status.conditions on CredentialsRequest CRs to identify the stale one(s).
          summary: One or more CredentialsRequest CRs are stale and should be deleted.
        expr: cco_credentials_requests_conditions{condition="StaleCredentials"} > 0
        for: 5m
        labels:
          severity: warning
  openshift-cluster-machine-approver-machineapprover-rules-c32204e6-8c48-458e-ac3a-eccfadee0566.yaml: |
    groups:
    - name: cluster-machine-approver.rules
      rules:
      - alert: MachineApproverMaxPendingCSRsReached
        annotations:
          description: |
            The number of pending CertificateSigningRequests has exceeded the
            maximum threshold (current number of machine + 100). Check the
            pending CSRs to determine which machines need approval, also check
            that the nodelink controller is running in the openshift-machine-api
            namespace.
          summary: max pending CSRs threshold reached.
        expr: |
          mapi_current_pending_csr > mapi_max_pending_csr
        for: 5m
        labels:
          severity: warning
  openshift-cluster-node-tuning-operator-node-tuning-operator-d2d3b625-644c-47f8-b778-6d719743f28a.yaml: |
    groups:
    - name: node-tuning-operator.rules
      rules:
      - alert: NTOPodsNotReady
        annotations:
          description: |
            Pod {{ $labels.pod }} is not ready.
            Review the "Event" objects in "openshift-cluster-node-tuning-operator" namespace for further details.
          summary: Pod {{ $labels.pod }} is not ready.
        expr: |
          kube_pod_status_ready{namespace='openshift-cluster-node-tuning-operator', condition='true'} == 0
        for: 30m
        labels:
          severity: warning
      - alert: NTODegraded
        annotations:
          description: The Node Tuning Operator is degraded. Review the "node-tuning"
            ClusterOperator object for further details.
          summary: The Node Tuning Operator is degraded.
        expr: nto_degraded_info == 1
        for: 2h
        labels:
          severity: warning
      - expr: count by (_id) (nto_profile_calculated_total{profile!~"openshift-node",profile!~"openshift-control-plane",profile!~"openshift"})
        record: nto_custom_profiles:count
  openshift-cluster-samples-operator-samples-operator-alerts-f654ce69-ca3a-4d68-833a-2925ec3688ca.yaml: |
    groups:
    - name: SamplesOperator
      rules:
      - alert: SamplesRetriesMissingOnImagestreamImportFailing
        annotations:
          description: |
            Samples operator is detecting problems with imagestream image imports, and the periodic retries of those
            imports are not occurring.  Contact support.  You can look at the "openshift-samples" ClusterOperator object
            for details. Most likely there are issues with the external image registry hosting the images that need to
            be investigated.  The list of ImageStreams that have failing imports are:
            {{ range query "openshift_samples_failed_imagestream_import_info > 0" }}
              {{ .Labels.name }}
            {{ end }}
            However, the list of ImageStreams for which samples operator is retrying imports is:
            retrying imports:
            {{ range query "openshift_samples_retry_imagestream_import_total > 0" }}
               {{ .Labels.imagestreamname }}
            {{ end }}
          summary: Samples operator is having problems with imagestream imports and its
            retries.
        expr: sum(openshift_samples_failed_imagestream_import_info) > sum(openshift_samples_retry_imagestream_import_total)
          - sum(openshift_samples_retry_imagestream_import_total offset 30m)
        for: 2h
        labels:
          namespace: openshift-cluster-samples-operator
          severity: warning
      - alert: SamplesImagestreamImportFailing
        annotations:
          description: |
            Samples operator is detecting problems with imagestream image imports.  You can look at the "openshift-samples"
            ClusterOperator object for details. Most likely there are issues with the external image registry hosting
            the images that needs to be investigated.  Or you can consider marking samples operator Removed if you do not
            care about having sample imagestreams available.  The list of ImageStreams for which samples operator is
            retrying imports:
            {{ range query "openshift_samples_retry_imagestream_import_total > 0" }}
               {{ .Labels.imagestreamname }}
            {{ end }}
          summary: Samples operator is detecting problems with imagestream image imports
        expr: sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total
          offset 30m) > sum(openshift_samples_failed_imagestream_import_info)
        for: 2h
        labels:
          namespace: openshift-cluster-samples-operator
          severity: warning
      - alert: SamplesDegraded
        annotations:
          description: |
            Samples could not be deployed and the operator is degraded. Review the "openshift-samples" ClusterOperator object for further details.
          summary: Samples operator is degraded.
        expr: openshift_samples_degraded_info == 1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesInvalidConfig
        annotations:
          description: |
            Samples operator has been given an invalid configuration.
          summary: Samples operator Invalid configuration
        expr: openshift_samples_invalidconfig_info == 1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesMissingSecret
        annotations:
          description: |
            Samples operator cannot find the samples pull secret in the openshift namespace.
          summary: Samples operator is not able to find secret
        expr: openshift_samples_invalidsecret_info{reason="missing_secret"} == 1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesMissingTBRCredential
        annotations:
          description: |
            The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.
          summary: Samples operator is not able to find the credentials for registry
        expr: openshift_samples_invalidsecret_info{reason="missing_tbr_credential"} ==
          1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesTBRInaccessibleOnBoot
        annotations:
          description: |
            One of two situations has occurred.  Either
            samples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed.
            If this is expected, and stems from installing in a restricted network environment, please note that if you
            plan on mirroring images associated with sample imagestreams into a registry available in your restricted
            network environment, and subsequently moving samples operator back to 'Managed' state, a list of the images
            associated with each image stream tag from the samples catalog is
            provided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to
            assist the mirroring process.
            Or, the use of allowed registries or blocked registries with global imagestream configuration will not allow
            samples operator to create imagestreams using the default image registry 'registry.redhat.io'.
          summary: Samples operator is not able to access the registry on boot
        expr: openshift_samples_tbr_inaccessible_info == 1
        for: 2d
        labels:
          severity: info
  openshift-cluster-storage-operator-prometheus-0bca8690-4373-405d-bc81-b874e64c76e9.yaml: |
    groups:
    - name: default-storage-classes.rules
      rules:
      - alert: MultipleDefaultStorageClasses
        annotations:
          description: |
            Cluster storage operator monitors all storage classes configured in the cluster
            and checks there is not more than one default StorageClass configured.
          message: StorageClass count check is failing (there should not be more than
            one default StorageClass)
          summary: More than one default StorageClass detected.
        expr: max_over_time(default_storage_class_count[5m]) > 1
        for: 10m
        labels:
          severity: critical
    - name: storage-operations.rules
      rules:
      - alert: PodStartupStorageOperationsFailing
        annotations:
          description: |
            Failing storage operation "{{ $labels.operation_name }}" of volume plugin {{ $labels.volume_plugin }} was preventing Pods on node {{ $labels.node }}
            from starting for past 5 minutes.
            Please investigate Pods that are "ContainerCreating" on the node: "oc get pod --field-selector=spec.nodeName={{ $labels.node }} --all-namespaces | grep ContainerCreating".
            Events of the Pods should contain exact error message: "oc describe pod -n <pod namespace> <pod name>".
          summary: Pods can't start because {{ $labels.operation_name }} of volume plugin
            {{ $labels.volume_plugin }} is permanently failing on node {{ $labels.node
            }}.
        expr: |
          increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) > 0
          and on() increase(storage_operation_duration_seconds_count{status = "success", operation_name =~"volume_attach|volume_mount"}[5m]) == 0
        for: 5m
        labels:
          severity: info
  openshift-cluster-version-cluster-version-operator-d6ae0489-62c1-4fee-b48f-91d1b937017b.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: ClusterVersionOperatorDown
        annotations:
          description: The operator may be down or disabled. The cluster will not be kept
            up to date and upgrades will not be possible. Inspect the openshift-cluster-version
            namespace for events or changes to the cluster-version-operator deployment
            or pods to diagnose and repair. {{ with $console_url := "console_url" | query
            }}{{ if ne (len (label "url" (first $console_url ) ) ) 0}} For more information
            refer to {{ label "url" (first $console_url ) }}/k8s/cluster/projects/openshift-cluster-version.{{
            end }}{{ end }}
          summary: Cluster version operator has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="cluster-version-operator"} == 1)
        for: 10m
        labels:
          namespace: openshift-cluster-version
          severity: critical
      - alert: CannotRetrieveUpdates
        annotations:
          description: Failure to retrieve updates means that cluster administrators will
            need to monitor for available updates on their own or risk falling behind
            on security or other bugfixes. If the failure is expected, you can clear spec.channel
            in the ClusterVersion object to tell the cluster-version operator to not retrieve
            updates. Failure reason {{ with $cluster_operator_conditions := "cluster_operator_conditions"
            | query}}{{range $value := .}}{{if and (eq (label "name" $value) "version")
            (eq (label "condition" $value) "RetrievedUpdates") (eq (label "endpoint" $value)
            "metrics") (eq (value $value) 0.0)}}{{label "reason" $value}} {{end}}{{end}}{{end}}.
            {{ with $console_url := "console_url" | query }}{{ if ne (len (label "url"
            (first $console_url ) ) ) 0}} For more information refer to {{ label "url"
            (first $console_url ) }}/settings/cluster/.{{ end }}{{ end }}
          summary: Cluster version operator has not retrieved updates in {{ $value | humanizeDuration
            }}.
        expr: "max by (namespace)\n(\n  (\n    time()-cluster_version_operator_update_retrieval_timestamp_seconds\n
          \ ) >= 3600 \n  and ignoring(condition, name, reason) \n  (cluster_operator_conditions{name=\"version\",
          condition=\"RetrievedUpdates\", endpoint=\"metrics\", reason!=\"NoChannel\"})\n)\n"
        labels:
          severity: warning
      - alert: UpdateAvailable
        annotations:
          description: For more information refer to 'oc adm upgrade'{{ with $console_url
            := "console_url" | query }}{{ if ne (len (label "url" (first $console_url
            ) ) ) 0}} or {{ label "url" (first $console_url ) }}/settings/cluster/{{ end
            }}{{ end }}.
          summary: Your upstream update recommendation service recommends you update your
            cluster.
        expr: |
          sum by (channel, namespace, upstream) (cluster_version_available_updates) > 0
        labels:
          severity: info
      - alert: ClusterReleaseNotAccepted
        annotations:
          description: The desired cluster release has not been accepted because ${{ $labels.reason
            }}, and the cluster will continue to reconcile an earlier release instead
            of moving towards that desired release.  For more information refer to 'oc
            adm upgrade'{{ with $console_url := "console_url" | query }}{{ if ne (len
            (label "url" (first $console_url ) ) ) 0}} or {{ label "url" (first $console_url
            ) }}/settings/cluster/{{ end }}{{ end }}.
          summary: The desired cluster release has not been accepted for at least an hour.
        expr: |
          max by (namespace, name, reason) (cluster_operator_conditions{name="version", condition="ReleaseAccepted", endpoint="metrics"} == 0)
        for: 60m
        labels:
          severity: warning
    - name: cluster-operators
      rules:
      - alert: ClusterNotUpgradeable
        annotations:
          description: In most cases, you will still be able to apply patch releases.
            Reason {{ with $cluster_operator_conditions := "cluster_operator_conditions"
            | query}}{{range $value := .}}{{if and (eq (label "name" $value) "version")
            (eq (label "condition" $value) "Upgradeable") (eq (label "endpoint" $value)
            "metrics") (eq (value $value) 0.0) (ne (len (label "reason" $value)) 0) }}{{label
            "reason" $value}}.{{end}}{{end}}{{end}} For more information refer to 'oc
            adm upgrade'{{ with $console_url := "console_url" | query }}{{ if ne (len
            (label "url" (first $console_url ) ) ) 0}} or {{ label "url" (first $console_url
            ) }}/settings/cluster/{{ end }}{{ end }}.
          summary: One or more cluster operators have been blocking minor version cluster
            upgrades for at least an hour.
        expr: |
          max by (namespace, name, condition, endpoint) (cluster_operator_conditions{name="version", condition="Upgradeable", endpoint="metrics"} == 0)
        for: 60m
        labels:
          severity: info
      - alert: ClusterOperatorDown
        annotations:
          description: The {{ $labels.name }} operator may be down or disabled because
            ${{ $labels.reason }}, and the components it manages may be unavailable or
            degraded.  Cluster upgrades may not complete. For more information refer to
            'oc get -o yaml clusteroperator {{ $labels.name }}'{{ with $console_url :=
            "console_url" | query }}{{ if ne (len (label "url" (first $console_url ) )
            ) 0}} or {{ label "url" (first $console_url ) }}/settings/cluster/{{ end }}{{
            end }}.
          summary: Cluster operator has not been available for 10 minutes.
        expr: |
          max by (namespace, name, reason) (cluster_operator_up{job="cluster-version-operator"} == 0)
        for: 10m
        labels:
          severity: critical
      - alert: ClusterOperatorDegraded
        annotations:
          description: The {{ $labels.name }} operator is degraded because {{ $labels.reason
            }}, and the components it manages may have reduced quality of service.  Cluster
            upgrades may not complete. For more information refer to 'oc get -o yaml clusteroperator
            {{ $labels.name }}'{{ with $console_url := "console_url" | query }}{{ if ne
            (len (label "url" (first $console_url ) ) ) 0}} or {{ label "url" (first $console_url
            ) }}/settings/cluster/{{ end }}{{ end }}.
          summary: Cluster operator has been degraded for 30 minutes.
        expr: |
          max by (namespace, name, reason)
          (
            (
              cluster_operator_conditions{job="cluster-version-operator", condition="Degraded"}
              or on (namespace, name)
              group by (namespace, name) (cluster_operator_up{job="cluster-version-operator"})
            ) == 1
          )
        for: 30m
        labels:
          severity: warning
      - alert: ClusterOperatorFlapping
        annotations:
          description: The  {{ $labels.name }} operator behavior might cause upgrades
            to be unstable. For more information refer to 'oc get -o yaml clusteroperator
            {{ $labels.name }}'{{ with $console_url := "console_url" | query }}{{ if ne
            (len (label "url" (first $console_url ) ) ) 0}} or {{ label "url" (first $console_url
            ) }}/settings/cluster/{{ end }}{{ end }}.
          summary: Cluster operator up status is changing often.
        expr: |
          max by (namespace, name) (changes(cluster_operator_up{job="cluster-version-operator"}[2m]) > 2)
        for: 10m
        labels:
          severity: warning
  openshift-dns-operator-dns-d1915c24-0808-41bf-a3c3-34c1ec5f3bb3.yaml: |
    groups:
    - name: openshift-dns.rules
      rules:
      - alert: CoreDNSPanicking
        annotations:
          description: '{{ $value }} CoreDNS panics observed on {{ $labels.instance }}'
          summary: CoreDNS panic
        expr: increase(coredns_panics_total[10m]) > 0
        for: 5m
        labels:
          severity: warning
      - alert: CoreDNSHealthCheckSlow
        annotations:
          description: CoreDNS Health Checks are slowing down (instance {{ $labels.instance
            }})
          summary: CoreDNS health checks
        expr: histogram_quantile(.95, sum(rate(coredns_health_request_duration_seconds_bucket[5m]))
          by (instance, le)) > 10
        for: 5m
        labels:
          severity: warning
      - alert: CoreDNSErrorsHigh
        annotations:
          description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage
            }} of requests.
          summary: CoreDNS serverfail
        expr: |
          (sum by(namespace) (rate(coredns_dns_responses_total{rcode="SERVFAIL"}[5m]))
            /
          sum by(namespace) (rate(coredns_dns_responses_total[5m])))
          > 0.01
        for: 5m
        labels:
          severity: warning
  openshift-etcd-operator-etcd-prometheus-rules-b5fadeaa-2fb9-44c3-a45f-544ca4464c86.yaml: |
    groups:
    - name: etcd
      rules:
      - alert: etcdMembersDown
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": members are down ({{ $value
            }}).'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdMembersDown.md
          summary: etcd cluster members are down.
        expr: |
          max without (endpoint) (
            sum without (instance) (up{job=~".*etcd.*"} == bool 0)
          or
            count without (To) (
              sum without (instance) (rate(etcd_network_peer_sent_failures_total{job=~".*etcd.*"}[120s])) > 0.01
            )
          )
          > 0
        for: 10m
        labels:
          severity: critical
      - alert: etcdNoLeader
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }}
            has no leader.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdNoLeader.md
          summary: etcd cluster has no leader.
        expr: |
          etcd_server_has_leader{job=~".*etcd.*"} == 0
        for: 1m
        labels:
          severity: critical
      - alert: etcdMemberCommunicationSlow
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": member communication with {{
            $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance
            }}.'
          summary: etcd cluster member communication is slow.
        expr: |
          histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighNumberOfFailedProposals
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures
            within the last 30 minutes on etcd instance {{ $labels.instance }}.'
          summary: etcd cluster has high number of proposal failures.
        expr: |
          rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
        for: 15m
        labels:
          severity: warning
      - alert: etcdHighFsyncDurations
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": 99th percentile fsync durations
            are {{ $value }}s on etcd instance {{ $labels.instance }}.'
          summary: etcd cluster 99th percentile fsync durations are too high.
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.5
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighFsyncDurations
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": 99th percentile fsync durations
            are {{ $value }}s on etcd instance {{ $labels.instance }}.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdHighFsyncDurations.md
          summary: etcd cluster 99th percentile fsync durations are too high.
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 1
        for: 10m
        labels:
          severity: critical
      - alert: etcdHighCommitDurations
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": 99th percentile commit durations
            {{ $value }}s on etcd instance {{ $labels.instance }}.'
          summary: etcd cluster 99th percentile commit durations are too high.
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.25
        for: 10m
        labels:
          severity: warning
      - alert: etcdDatabaseQuotaLowSpace
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": database size exceeds the defined
            quota on etcd instance {{ $labels.instance }}, please defrag or increase the
            quota as the writes to etcd will be disabled when it is full.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdDatabaseQuotaLowSpace.md
          summary: etcd cluster database is running full.
        expr: |
          (last_over_time(etcd_mvcc_db_total_size_in_bytes[5m]) / last_over_time(etcd_server_quota_backend_bytes[5m]))*100 > 95
        for: 10m
        labels:
          severity: critical
      - alert: etcdExcessiveDatabaseGrowth
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": Predicting running out of disk
            space in the next four hours, based on write observations within the past
            four hours on etcd instance {{ $labels.instance }}, please check as it might
            be disruptive.'
          summary: etcd cluster database growing very fast.
        expr: |
          predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 4*60*60) > etcd_server_quota_backend_bytes
        for: 10m
        labels:
          severity: warning
      - alert: etcdDatabaseHighFragmentationRatio
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": database size in use on instance
            {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual
            allocated disk space, please run defragmentation (e.g. etcdctl defrag) to
            retrieve the unused fragmented disk space.'
          runbook_url: https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation
          summary: etcd database size in use is less than 50% of the actual allocated
            storage.
        expr: |
          (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes[5m])) < 0.5
        for: 10m
        labels:
          severity: warning
    - name: openshift-etcd.rules
      rules:
      - alert: etcdGRPCRequestsSlow
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": 99th percentile of gRPC requests
            is {{ $value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method
            }} method.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdGRPCRequestsSlow.md
          summary: etcd grpc requests are slow
        expr: |
          histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job="etcd", grpc_method!="Defragment", grpc_type="unary"}[10m])) without(grpc_type))
          > 1
        for: 30m
        labels:
          severity: critical
      - alert: etcdHighNumberOfFailedGRPCRequests
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for
            {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
          summary: etcd cluster has high number of failed grpc requests.
        expr: |
          (sum(rate(grpc_server_handled_total{job="etcd", grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded"}[5m])) without (grpc_type, grpc_code)
            /
          (sum(rate(grpc_server_handled_total{job="etcd"}[5m])) without (grpc_type, grpc_code)
            > 2 and on ()(sum(cluster_infrastructure_provider{type!~"ipi|BareMetal"} == bool 1)))) * 100 > 10
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighNumberOfFailedGRPCRequests
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for
            {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdHighNumberOfFailedGRPCRequests.md
          summary: etcd cluster has high number of failed grpc requests.
        expr: |
          (sum(rate(grpc_server_handled_total{job="etcd", grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded"}[5m])) without (grpc_type, grpc_code)
            /
          (sum(rate(grpc_server_handled_total{job="etcd"}[5m])) without (grpc_type, grpc_code)
            > 2 and on ()(sum(cluster_infrastructure_provider{type!~"ipi|BareMetal"} == bool 1)))) * 100 > 50
        for: 10m
        labels:
          severity: critical
      - alert: etcdHighNumberOfLeaderChanges
        annotations:
          description: 'etcd cluster "{{ $labels.job }}": {{ $value }} average leader
            changes within the last 10 minutes. Frequent elections may be a sign of insufficient
            resources, high network latency, or disruptions by other components and should
            be investigated.'
          summary: etcd cluster has high number of leader changes.
        expr: |
          avg(changes(etcd_server_is_leader[10m])) > 5
        for: 5m
        labels:
          severity: warning
      - alert: etcdInsufficientMembers
        annotations:
          description: etcd is reporting fewer instances are available than are needed
            ({{ $value }}). When etcd does not have a majority of instances available
            the Kubernetes and OpenShift APIs will reject read and write requests and
            operations that preserve the health of workloads cannot be performed. This
            can occur when multiple control plane nodes are powered off or are unable
            to connect to each other via the network. Check that all control plane nodes
            are powered on and that network connections between each machine are functional.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdInsufficientMembers.md
          summary: etcd is reporting that a majority of instances are unavailable.
        expr: sum(up{job="etcd"} == bool 1 and etcd_server_has_leader{job="etcd"} == bool
          1) without (instance,pod) < ((count(up{job="etcd"}) without (instance,pod) +
          1) / 2)
        for: 3m
        labels:
          severity: critical
  openshift-image-registry-image-registry-rules-5b09a18c-3fb4-4371-889c-4504ff6ad1fd.yaml: |
    groups:
    - name: imageregistry.operations.rules
      rules:
      - expr: |
          label_replace(
            label_replace(
              sum by (operation) (imageregistry_request_duration_seconds_count{operation="BlobStore.ServeBlob"}), "operation", "get", "operation", "(.+)"
            ), "resource_type", "blob", "resource_type", ""
          )
        record: imageregistry:operations_count:sum
      - expr: |
          label_replace(
            label_replace(
              sum by (operation) (imageregistry_request_duration_seconds_count{operation="BlobStore.Create"}), "operation", "create", "operation", "(.+)"
            ), "resource_type", "blob", "resource_type", ""
          )
        record: imageregistry:operations_count:sum
      - expr: |
          label_replace(
            label_replace(
              sum by (operation) (imageregistry_request_duration_seconds_count{operation="ManifestService.Get"}), "operation", "get", "operation", "(.+)"
            ), "resource_type", "manifest", "resource_type", ""
          )
        record: imageregistry:operations_count:sum
      - expr: |
          label_replace(
            label_replace(
              sum by (operation) (imageregistry_request_duration_seconds_count{operation="ManifestService.Put"}), "operation", "create", "operation", "(.+)"
            ), "resource_type", "manifest", "resource_type", ""
          )
        record: imageregistry:operations_count:sum
  openshift-image-registry-imagestreams-rules-090cce58-af50-40b4-b2b3-bb81c8d4d65d.yaml: |
    groups:
    - name: imagestreams.rules
      rules:
      - expr: sum by (location, source) (image_registry_image_stream_tags_total)
        record: imageregistry:imagestreamtags_count:sum
  openshift-ingress-operator-ingress-operator-29c263a6-62d8-47f9-9ba3-ebcb6fd7b240.yaml: |
    groups:
    - name: openshift-ingress.rules
      rules:
      - alert: HAProxyReloadFail
        annotations:
          description: This alert fires when HAProxy fails to reload its configuration,
            which will result in the router not picking up recently created or modified
            routes.
          message: HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting
            recently created or modified routes
          summary: HAProxy reload failure
        expr: template_router_reload_failure == 1
        for: 5m
        labels:
          severity: warning
      - alert: HAProxyDown
        annotations:
          description: This alert fires when metrics report that HAProxy is down.
          message: HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace
            }} / {{ $labels.pod }}
          summary: HAProxy is down
        expr: haproxy_up == 0
        for: 5m
        labels:
          severity: critical
      - alert: IngressControllerDegraded
        annotations:
          description: This alert fires when the IngressController status is degraded.
          message: |
            The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is
            degraded: {{ $labels.reason }}.
          summary: IngressController is degraded
        expr: ingress_controller_conditions{condition="Degraded"} == 1
        for: 5m
        labels:
          severity: warning
      - alert: IngressControllerUnavailable
        annotations:
          description: This alert fires when the IngressController is not available.
          message: |
            The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is
            unavailable: {{ $labels.reason }}.
          summary: IngressController is unavailable
        expr: ingress_controller_conditions{condition="Available"} == 0
        for: 5m
        labels:
          severity: warning
      - expr: min(route_metrics_controller_routes_per_shard)
        record: cluster:route_metrics_controller_routes_per_shard:min
      - expr: max(route_metrics_controller_routes_per_shard)
        record: cluster:route_metrics_controller_routes_per_shard:max
      - expr: avg(route_metrics_controller_routes_per_shard)
        record: cluster:route_metrics_controller_routes_per_shard:avg
      - expr: quantile(0.5, route_metrics_controller_routes_per_shard)
        record: cluster:route_metrics_controller_routes_per_shard:median
      - expr: sum (openshift_route_info) by (tls_termination)
        record: cluster:openshift_route_info:tls_termination:sum
    - name: openshift-ingress-to-route-controller.rules
      rules:
      - alert: IngressWithoutClassName
        annotations:
          description: This alert fires when there is an Ingress with an unset IngressClassName
            for longer than one day.
          message: Ingress {{ $labels.name }} is missing the IngressClassName for 1 day.
          summary: Ingress without IngressClassName for 1 day
        expr: openshift_ingress_to_route_controller_ingress_without_class_name == 1
        for: 1d
        labels:
          severity: warning
      - alert: UnmanagedRoutes
        annotations:
          description: This alert fires when there is a Route owned by an unmanaged Ingress.
          message: Route {{ $labels.name }} is owned by an unmanaged Ingress.
          summary: Route owned by an Ingress no longer managed
        expr: openshift_ingress_to_route_controller_route_with_unmanaged_owner == 1
        for: 1h
        labels:
          severity: warning
  openshift-insights-insights-prometheus-rules-589fecec-3db0-40b5-bddd-8b99cd7ee43d.yaml: |
    groups:
    - name: insights
      rules:
      - alert: InsightsDisabled
        annotations:
          description: 'Insights operator is disabled. In order to enable Insights and
            benefit from recommendations specific to your cluster, please follow steps
            listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html'
          summary: Insights operator is disabled.
        expr: |
          max without (job, pod, service, instance) (cluster_operator_conditions{name="insights", condition="Disabled"} == 1)
        for: 5m
        labels:
          namespace: openshift-insights
          severity: info
      - alert: SimpleContentAccessNotAvailable
        annotations:
          description: Simple content access (SCA) is not enabled. Once enabled, Insights
            Operator can automatically import the SCA certificates from Red Hat OpenShift
            Cluster Manager making it easier to use the content provided by your Red Hat
            subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html
            for more information.
          summary: Simple content access certificates are not available.
        expr: |
          max without (job, pod, service, instance) (max_over_time(cluster_operator_conditions{name="insights", condition="SCAAvailable", reason="NotFound"}[5m]) == 0)
        for: 5m
        labels:
          namespace: openshift-insights
          severity: info
      - alert: InsightsRecommendationActive
        annotations:
          description: Insights recommendation "{{ $labels.description }}" with total
            risk "{{ $labels.total_risk }}" was detected on the cluster. More information
            is available at {{ $labels.info_link }}.
          summary: An Insights recommendation is active for this cluster.
        expr: |
          insights_recommendation_active == 1
        for: 5m
        labels:
          severity: info
  openshift-kube-apiserver-api-usage-b27df59b-68ea-4552-accd-fd206e6b9543.yaml: |
    groups:
    - name: pre-release-lifecycle
      rules:
      - alert: APIRemovedInNextReleaseInUse
        annotations:
          description: Deprecated API that will be removed in the next version is being
            used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version
            }}/{{ $labels.resource }} API might be necessary for a successful upgrade
            to the next cluster version with Kubernetes {{ $labels.removed_release }}.
            Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version
            }}.{{ $labels.group }} -o yaml` to identify the workload.
          summary: Deprecated API that will be removed in the next version is being used.
        expr: group by (group,version,resource,removed_release) (apiserver_requested_deprecated_apis{removed_release="1.27"})
          * on (group,version,resource) group_left () sum by (group,version,resource)
          ( rate(apiserver_request_total{system_client!="kube-controller-manager",system_client!="cluster-policy-controller"}[4h])
          ) > 0
        for: 1h
        labels:
          namespace: openshift-kube-apiserver
          severity: info
      - alert: APIRemovedInNextEUSReleaseInUse
        annotations:
          description: Deprecated API that will be removed in the next EUS version is
            being used. Removing the workload that is using the {{ $labels.group }}.{{
            $labels.version }}/{{ $labels.resource }} API might be necessary for a successful
            upgrade to the next EUS cluster version with Kubernetes {{ $labels.removed_release
            }}. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version
            }}.{{ $labels.group }} -o yaml` to identify the workload.
          summary: Deprecated API that will be removed in the next EUS version is being
            used.
        expr: group by (group,version,resource,removed_release) (apiserver_requested_deprecated_apis{removed_release=~"1[.]2[7]"})
          * on (group,version,resource) group_left () sum by (group,version,resource)
          ( rate(apiserver_request_total{system_client!="kube-controller-manager",system_client!="cluster-policy-controller"}[4h])
          ) > 0
        for: 1h
        labels:
          namespace: openshift-kube-apiserver
          severity: info
  openshift-kube-apiserver-audit-errors-870f666d-bfce-4ed2-81bd-c508887d2478.yaml: |
    groups:
    - name: apiserver-audit
      rules:
      - alert: AuditLogError
        annotations:
          description: An API Server had an error writing to an audit log.
          summary: |-
            An API Server instance was unable to write audit logs. This could be
            triggered by the node running out of space, or a malicious actor
            tampering with the audit logs.
        expr: |
          sum by (apiserver,instance)(rate(apiserver_audit_error_total{apiserver=~".+-apiserver"}[5m])) / sum by (apiserver,instance) (rate(apiserver_audit_event_total{apiserver=~".+-apiserver"}[5m])) > 0
        for: 1m
        labels:
          namespace: openshift-kube-apiserver
          severity: warning
  openshift-kube-apiserver-cpu-utilization-4f8ebedc-3a83-4194-878e-d72f422f337f.yaml: |
    groups:
    - name: control-plane-cpu-utilization
      rules:
      - alert: HighOverallControlPlaneCPU
        annotations:
          description: Given three control plane nodes, the overall CPU utilization may
            only be about 2/3 of all available capacity. This is because if a single control
            plane node fails, the remaining two must handle the load of the cluster in
            order to be HA. If the cluster is using more than 2/3 of all capacity, if
            one control plane node fails, the remaining two are likely to fail when they
            take the load. To fix this, increase the CPU and memory on your control plane
            nodes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md
          summary: CPU utilization across all three control plane nodes is higher than
            two control plane nodes can sustain; a single control plane node outage may
            cause a cascading failure; increase available CPU.
        expr: |
          sum(
            100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100)
            AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          )
          /
          count(kube_node_role{role="master"})
          > 60
        for: 10m
        labels:
          namespace: openshift-kube-apiserver
          severity: warning
      - alert: ExtremelyHighIndividualControlPlaneCPU
        annotations:
          description: Extreme CPU pressure can cause slow serialization and poor performance
            from the kube-apiserver and etcd. When this happens, there is a risk of clients
            seeing non-responsive API requests which are issued again causing even more
            CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness
            on the backend. If one kube-apiserver fails under this condition, chances
            are you will experience a cascade as the remaining kube-apiservers are also
            under-provisioned. To fix this, increase the CPU and memory on your control
            plane nodes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md
          summary: CPU utilization on a single control plane node is very high, more CPU
            pressure is likely to cause a failover; increase available CPU.
        expr: |
          100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 90 AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
        for: 5m
        labels:
          namespace: openshift-kube-apiserver
          severity: warning
      - alert: ExtremelyHighIndividualControlPlaneCPU
        annotations:
          description: Extreme CPU pressure can cause slow serialization and poor performance
            from the kube-apiserver and etcd. When this happens, there is a risk of clients
            seeing non-responsive API requests which are issued again causing even more
            CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness
            on the backend. If one kube-apiserver fails under this condition, chances
            are you will experience a cascade as the remaining kube-apiservers are also
            under-provisioned. To fix this, increase the CPU and memory on your control
            plane nodes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md
          summary: Sustained high CPU utilization on a single control plane node, more
            CPU pressure is likely to cause a failover; increase available CPU.
        expr: |
          100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 90 AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
        for: 1h
        labels:
          namespace: openshift-kube-apiserver
          severity: critical
  openshift-kube-apiserver-kube-apiserver-requests-ae382c5b-05c1-4ef5-85a6-7e1b8e10361b.yaml: |
    groups:
    - name: apiserver-requests-in-flight
      rules:
      - expr: |
          max_over_time(sum(apiserver_current_inflight_requests{apiserver=~"openshift-apiserver|kube-apiserver"}) by (apiserver,requestKind)[2m:])
        record: cluster:apiserver_current_inflight_requests:sum:max_over_time:2m
  openshift-kube-apiserver-kube-apiserver-slos-basic-c2e5e4c9-bfac-48c9-9623-b289ac4931a3.yaml: |
    groups:
    - name: kube-apiserver-slos-basic
      rules:
      - alert: KubeAPIErrorBudgetBurn
        annotations:
          description: The API server is burning too much error budget. This alert fires
            when too many requests are failing with high latency. Use the 'API Performance'
            monitoring dashboards to narrow down the request states and latency. The 'etcd'
            monitoring dashboards also provides metrics to help determine etcd stability
            and performance.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md
          summary: The API server is burning too much error budget.
        expr: |
          sum(apiserver_request:burnrate1h) > (14.40 * 0.01000)
          and
          sum(apiserver_request:burnrate5m) > (14.40 * 0.01000)
        for: 2m
        labels:
          long: 1h
          namespace: openshift-kube-apiserver
          severity: critical
          short: 5m
      - alert: KubeAPIErrorBudgetBurn
        annotations:
          description: The API server is burning too much error budget. This alert fires
            when too many requests are failing with high latency. Use the 'API Performance'
            monitoring dashboards to narrow down the request states and latency. The 'etcd'
            monitoring dashboards also provides metrics to help determine etcd stability
            and performance.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md
          summary: The API server is burning too much error budget.
        expr: |
          sum(apiserver_request:burnrate6h) > (6.00 * 0.01000)
          and
          sum(apiserver_request:burnrate30m) > (6.00 * 0.01000)
        for: 15m
        labels:
          long: 6h
          namespace: openshift-kube-apiserver
          severity: critical
          short: 30m
    - name: kube-apiserver.rules
      rules:
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
              -
              (
                (
                  sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[5m]))
                  or
                  vector(0)
                )
                +
                sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[5m]))
                +
                sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[5m]))
              )
            )
            +
            # errors
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
        labels:
          verb: read
        record: apiserver_request:burnrate5m
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
              -
              (
                (
                  sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[30m]))
                  or
                  vector(0)
                )
                +
                sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[30m]))
                +
                sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[30m]))
              )
            )
            +
            # errors
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
        labels:
          verb: read
        record: apiserver_request:burnrate30m
      - expr: "(\n  (\n    # too slow\n    sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\",subresource!~\"proxy|attach|log|exec|portforward\"}[1h]))\n
          \   -\n    (\n      (\n        sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\",subresource!~\"proxy|attach|log|exec|portforward\",scope=~\"resource|\",le=\"1\"}[1h]))\n
          \       or\n        vector(0)\n      )\n      +\n      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\",subresource!~\"proxy|attach|log|exec|portforward\",scope=\"namespace\",le=\"5\"}[1h]))\n
          \     +\n      sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\",subresource!~\"proxy|attach|log|exec|portforward\",scope=\"cluster\",le=\"30\"}[1h]))\n
          \   )\n  )\n  +\n  # errors\n  sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\",code=~\"5..\"}[1h]))\n)\n/\nsum
          by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))
          \   \n"
        labels:
          verb: read
        record: apiserver_request:burnrate1h
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
              -
              (
                (
                  sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[6h]))
                  or
                  vector(0)
                )
                +
                sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[6h]))
                +
                sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[6h]))
              )
            )
            +
            # errors
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
        labels:
          verb: read
        record: apiserver_request:burnrate6h
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
              -
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1h]))
            )
            +
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
        labels:
          verb: write
        record: apiserver_request:burnrate1h
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
              -
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[30m]))
            )
            +
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
        labels:
          verb: write
        record: apiserver_request:burnrate30m
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
              -
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[5m]))
            )
            +
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
        labels:
          verb: write
        record: apiserver_request:burnrate5m
      - expr: |
          (
            (
              # too slow
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
              -
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[6h]))
            )
            +
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
          )
          /
          sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
        labels:
          verb: write
        record: apiserver_request:burnrate6h
      - expr: |
          sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
        labels:
          verb: read
        record: code_resource:apiserver_request_total:rate5m
      - expr: |
          sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
        labels:
          verb: write
        record: code_resource:apiserver_request_total:rate5m
      - expr: |
          histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET"}[5m]))) > 0
        labels:
          quantile: "0.99"
          verb: read
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))) > 0
        labels:
          quantile: "0.99"
          verb: write
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
  openshift-kube-apiserver-operator-kube-apiserver-operator-a0ceb7db-1588-49c5-b7ef-fab47a36997d.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: TechPreviewNoUpgrade
        annotations:
          description: Cluster has enabled Technology Preview features that cannot be
            undone and will prevent upgrades. The TechPreviewNoUpgrade feature set is
            not recommended on production clusters.
          summary: Cluster has enabled tech preview features that will prevent upgrades.
        expr: |
          cluster_feature_set{name!="", namespace="openshift-kube-apiserver-operator"} == 0
        for: 10m
        labels:
          severity: warning
  openshift-kube-apiserver-podsecurity-3e9c2784-7f5d-4f68-8ad2-64013f60b605.yaml: |
    groups:
    - name: pod-security-violation
      rules:
      - alert: PodSecurityViolation
        annotations:
          description: A workload (pod, deployment, daemonset, ...) was created somewhere
            in the cluster but it did not match the PodSecurity "{{ $labels.policy_level
            }}" profile defined by its namespace either via the cluster-wide configuration
            (which triggers on a "restricted" profile violations) or by the namespace
            local Pod Security labels. Refer to Kubernetes documentation on Pod Security
            Admission to learn more about these violations.
          summary: One or more workloads users created in the cluster don't match their
            Pod Security profile
        expr: |
          sum(increase(pod_security_evaluations_total{decision="deny",mode="audit",resource="pod"}[1d])) by (policy_level) > 0
        labels:
          namespace: openshift-kube-apiserver
          severity: info
  openshift-kube-controller-manager-operator-kube-controller-manager-operator-eb63ef6b-5bd4-4fce-aabf-1a92eebe9141.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: KubeControllerManagerDown
        annotations:
          description: KubeControllerManager has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/KubeControllerManagerDown.md
          summary: Target disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-controller-manager"} == 1)
        for: 15m
        labels:
          namespace: openshift-kube-controller-manager
          severity: critical
      - alert: PodDisruptionBudgetAtLimit
        annotations:
          description: The pod disruption budget is at the minimum disruptions allowed
            level. The number of current healthy pods is equal to the desired healthy
            pods.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/PodDisruptionBudgetAtLimit.md
          summary: The pod disruption budget is preventing further disruption to pods.
        expr: |
          max by(namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_current_healthy == kube_poddisruptionbudget_status_desired_healthy and on (namespace, poddisruptionbudget) kube_poddisruptionbudget_status_expected_pods > 0)
        for: 60m
        labels:
          severity: warning
      - alert: PodDisruptionBudgetLimit
        annotations:
          description: The pod disruption budget is below the minimum disruptions allowed
            level and is not satisfied. The number of current healthy pods is less than
            the desired healthy pods.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/PodDisruptionBudgetLimit.md
          summary: The pod disruption budget registers insufficient amount of pods.
        expr: |
          max by (namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_current_healthy < kube_poddisruptionbudget_status_desired_healthy)
        for: 15m
        labels:
          severity: critical
      - alert: GarbageCollectorSyncFailed
        annotations:
          description: Garbage Collector had a problem with syncing and monitoring the
            available resources. Please see KubeControllerManager logs for more details.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/GarbageCollectorSyncFailed.md
          summary: There was a problem with syncing the resources for garbage collection.
        expr: |
          rate(garbagecollector_controller_resources_sync_error_total{}[5m]) > 0
        for: 60m
        labels:
          severity: warning
  openshift-kube-scheduler-operator-kube-scheduler-operator-f8105fa2-5cb4-491b-ac57-42b39164f6c1.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: KubeSchedulerDown
        annotations:
          description: KubeScheduler has disappeared from Prometheus target discovery.
          summary: Target disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="scheduler"} == 1)
        for: 15m
        labels:
          namespace: openshift-kube-scheduler
          severity: critical
    - name: scheduler-legacy-policy-deprecated
      rules:
      - alert: SchedulerLegacyPolicySet
        annotations:
          description: The scheduler is currently configured to use a legacy scheduler
            policy API. Use of the policy API is deprecated and removed in 4.10.
          summary: Legacy scheduler policy API in use by the scheduler.
        expr: |
          cluster_legacy_scheduler_policy > 0
        for: 60m
        labels:
          severity: warning
  openshift-machine-api-machine-api-operator-prometheus-rules-9acfe89f-501c-468b-b8dd-f2bec38999ad.yaml: |
    groups:
    - name: machine-without-valid-node-ref
      rules:
      - alert: MachineWithoutValidNode
        annotations:
          description: |
            If the machine never became a node, you should diagnose the machine related failures.
            If the node was deleted from the API, you may delete the machine if appropriate.
          summary: machine {{ $labels.name }} does not have valid node reference
        expr: |
          sum by (name, namespace) (mapi_machine_created_timestamp_seconds unless on(node) kube_node_info) > 0
        for: 60m
        labels:
          severity: warning
    - name: machine-with-no-running-phase
      rules:
      - alert: MachineWithNoRunningPhase
        annotations:
          description: |
            The machine has been without a Running or Deleting phase for more than 60 minutes.
            The machine may not have been provisioned properly from the infrastructure provider, or
            it might have issues with CertificateSigningRequests being approved.
          summary: 'machine {{ $labels.name }} is in phase: {{ $labels.phase }}'
        expr: |
          sum by (name, namespace) (mapi_machine_created_timestamp_seconds{phase!~"Running|Deleting"}) > 0
        for: 60m
        labels:
          severity: warning
    - name: machine-not-yet-deleted
      rules:
      - alert: MachineNotYetDeleted
        annotations:
          description: |
            The machine is not properly deleting, this may be due to a configuration issue with the
            infrastructure provider, or because workloads on the node have PodDisruptionBudgets or
            long termination periods which are preventing deletion.
          summary: machine {{ $labels.name }} has been in Deleting phase for more than
            6 hours
        expr: |
          sum by (name, namespace) (avg_over_time(mapi_machine_created_timestamp_seconds{phase="Deleting"}[15m])) > 0
        for: 360m
        labels:
          severity: warning
    - name: machine-api-operator-metrics-collector-up
      rules:
      - alert: MachineAPIOperatorMetricsCollectionFailing
        annotations:
          description: 'For more details:  oc logs <machine-api-operator-pod-name> -n
            openshift-machine-api'
          summary: machine api operator metrics collection is failing.
        expr: |
          mapi_mao_collector_up == 0
        for: 5m
        labels:
          severity: critical
    - name: machine-health-check-unterminated-short-circuit
      rules:
      - alert: MachineHealthCheckUnterminatedShortCircuit
        annotations:
          description: |
            The number of unhealthy machines has exceeded the `maxUnhealthy` limit for the check, you should check
            the status of machines in the cluster.
          summary: machine health check {{ $labels.name }} has been disabled by short
            circuit for more than 30 minutes
        expr: |
          mapi_machinehealthcheck_short_circuit == 1
        for: 30m
        labels:
          severity: warning
  openshift-machine-config-operator-machine-config-controller-75640bcc-d020-4e69-8e95-8f442b6ac00b.yaml: |
    groups:
    - name: mcc-paused-pool-kubelet-ca
      rules:
      - alert: MachineConfigControllerPausedPoolKubeletCA
        annotations:
          description: Machine config pools have a 'pause' feature, which allows config
            to be rendered, but prevents it from being rolled out to the nodes. This alert
            indicates that a certificate rotation has taken place, and the new kubelet-ca
            certificate bundle has been rendered into a machine config, but because the
            pool '{{$labels.pool}}' is paused, the config cannot be rolled out to the
            nodes in that pool. You will notice almost immediately that for nodes in pool
            '{{$labels.pool}}', pod logs will not be visible in the console and interactive
            commands (oc log, oc exec, oc debug, oc attach) will not work. You must unpause
            machine config pool '{{$labels.pool}}' to let the certificates through before
            the kube-apiserver-to-kubelet-signer certificate expires on {{ $value | humanizeTimestamp
            }} or this pool's nodes will cease to function properly.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/machine-config-operator/MachineConfigControllerPausedPoolKubeletCA.md
          summary: Paused machine configuration pool '{{$labels.pool}}' is blocking a
            necessary certificate rotation and must be unpaused before the current kube-apiserver-to-kubelet-signer
            certificate expires on {{ $value | humanizeTimestamp }}.
        expr: |
          max by (namespace,pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m])) > 0
        for: 60m
        labels:
          namespace: openshift-machine-config-operator
          severity: warning
      - alert: MachineConfigControllerPausedPoolKubeletCA
        annotations:
          description: Machine config pools have a 'pause' feature, which allows config
            to be rendered, but prevents it from being rolled out to the nodes. This alert
            indicates that a certificate rotation has taken place, and the new kubelet-ca
            certificate bundle has been rendered into a machine config, but because the
            pool '{{$labels.pool}}' is paused, the config cannot be rolled out to the
            nodes in that pool. You will notice almost immediately that for nodes in pool
            '{{$labels.pool}}', pod logs will not be visible in the console and interactive
            commands (oc log, oc exec, oc debug, oc attach) will not work. You must unpause
            machine config pool '{{$labels.pool}}' to let the certificates through before
            the kube-apiserver-to-kubelet-signer certificate expires. You have approximately
            {{ $value | humanizeDuration }} remaining before this happens and nodes in
            '{{$labels.pool}}' cease to function properly.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/machine-config-operator/MachineConfigControllerPausedPoolKubeletCA.md
          summary: Paused machine configuration pool '{{$labels.pool}}' is blocking a
            necessary certificate rotation and must be unpaused before the current kube-apiserver-to-kubelet-signer
            certificate expires in {{ $value | humanizeDuration }}.
        expr: |
          max by (namespace,pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m]) - time()) < (86400 * 14) AND max by (namespace,pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m])) > 0
        for: 60m
        labels:
          namespace: openshift-machine-config-operator
          severity: critical
    - name: os-image-override.rules
      rules:
      - expr: sum(os_image_url_override)
        record: os_image_url_override:sum
    - name: mcc-drain-error
      rules:
      - alert: MCCDrainError
        annotations:
          message: 'Drain failed on {{ $labels.exported_node }} , updates may be blocked.
            For more details check MachineConfigController pod logs: oc logs -f -n {{
            $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller'
        expr: |
          mcc_drain_err > 0
        labels:
          namespace: openshift-machine-config-operator
          severity: warning
  openshift-machine-config-operator-machine-config-daemon-de86511a-1cca-4a40-b83c-742dc4b71234.yaml: |
    groups:
    - name: mcd-reboot-error
      rules:
      - alert: MCDRebootError
        annotations:
          message: 'Reboot failed on {{ $labels.node }} , update may be blocked. For more
            details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod }} -c machine-config-daemon '
        expr: |
          mcd_reboots_failed_total > 0
        for: 5m
        labels:
          namespace: openshift-machine-config-operator
          severity: critical
    - name: mcd-pivot-error
      rules:
      - alert: MCDPivotError
        annotations:
          message: 'Error detected in pivot logs on {{ $labels.node }} , upgrade may be
            blocked. For more details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod
            }} -c machine-config-daemon '
        expr: |
          mcd_pivot_errors_total > 0
        for: 2m
        labels:
          namespace: openshift-machine-config-operator
          severity: warning
    - name: mcd-kubelet-health-state-error
      rules:
      - alert: KubeletHealthState
        annotations:
          message: Kubelet health failure threshold reached
        expr: |
          mcd_kubelet_state > 2
        labels:
          namespace: openshift-machine-config-operator
          severity: warning
    - name: system-memory-exceeds-reservation
      rules:
      - alert: SystemMemoryExceedsReservation
        annotations:
          message: System memory usage of {{ $value | humanize }} on {{ $labels.node }}
            exceeds 95% of the reservation. Reserved memory ensures system processes can
            function even when the node is fully allocated and protects against workload
            out of memory events impacting the proper functioning of the node. The default
            reservation is expected to be sufficient for most configurations and should
            be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html)
            when running nodes with high numbers of pods (either due to rate of change
            or at steady state).
        expr: |
          sum by (node) (container_memory_rss{id="/system.slice"}) > ((sum by (node) (kube_node_status_capacity{resource="memory"} - kube_node_status_allocatable{resource="memory"})) * 0.95)
        for: 15m
        labels:
          namespace: openshift-machine-config-operator
          severity: warning
    - name: high-overall-control-plane-memory
      rules:
      - alert: HighOverallControlPlaneMemory
        annotations:
          description: Given three control plane nodes, the overall memory utilization
            may only be about 2/3 of all available capacity. This is because if a single
            control plane node fails, the kube-apiserver and etcd my be slow to respond.
            To fix this, increase memory of the control plane nodes.
          summary: Memory utilization across all control plane nodes is high, and could
            impact responsiveness and stability.
        expr: |
          (
            1
            -
            sum (
              node_memory_MemFree_bytes
              + node_memory_Buffers_bytes
              + node_memory_Cached_bytes
              AND on (instance)
              label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
            ) / sum (
              node_memory_MemTotal_bytes
              AND on (instance)
              label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
            )
          ) * 100 > 60
        for: 1h
        labels:
          namespace: openshift-machine-config-operator
          severity: warning
    - name: extremely-high-individual-control-plane-memory
      rules:
      - alert: ExtremelyHighIndividualControlPlaneMemory
        annotations:
          description: The memory utilization per instance within control plane nodes
            influence the stability, and responsiveness of the cluster. This can lead
            to cluster instability and slow responses from kube-apiserver or failing requests
            specially on etcd. Moreover, OOM kill is expected which negatively influences
            the pod scheduling. If this happens on container level, the descheduler will
            not be able to detect it, as it works on the pod level. To fix this, increase
            memory of the affected node of control plane nodes.
          summary: Extreme memory utilization per node within control plane nodes is extremely
            high, and could impact responsiveness and stability.
        expr: |
          (
            1
            -
            sum by (instance) (
              node_memory_MemFree_bytes
              + node_memory_Buffers_bytes
              + node_memory_Cached_bytes
              AND on (instance)
              label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
            ) / sum by (instance) (
              node_memory_MemTotal_bytes
              AND on (instance)
              label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
            )
          ) * 100 > 90
        for: 45m
        labels:
          namespace: openshift-machine-config-operator
          severity: critical
  openshift-marketplace-marketplace-alert-rules-634b3e72-8dca-4164-a8e9-3d9444eaba00.yaml: |
    groups:
    - name: operator.marketplace.rules
      rules:
      - alert: OperatorHubSourceError
        annotations:
          description: Operators shipped via the {{ $labels.name }} source are not available
            for installation until the issue is fixed. Operators already installed from
            this source will not receive updates until issue is fixed. Inspect the status
            of the pod owned by {{ $labels.name }} source in the openshift-marketplace
            namespace (oc -n openshift-marketplace get pods -l olm.catalogSource={{ $labels.name
            }}) to diagnose and repair.
          summary: The {{ $labels.name }} source is in non-ready state for more than 10
            minutes.
        expr: catalogsource_ready{exported_namespace="openshift-marketplace"} == 0
        for: 10m
        labels:
          severity: warning
  openshift-monitoring-alertmanager-main-rules-79310b75-0593-4363-a06e-0753c8372101.yaml: |
    groups:
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerFailedReload
        annotations:
          description: Configuration has failed to load for {{ $labels.namespace }}/{{
            $labels.pod}}.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md
          summary: Reloading an Alertmanager configuration has failed.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(alertmanager_config_last_reload_successful{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) == 0
        for: 10m
        labels:
          severity: critical
      - alert: AlertmanagerMembersInconsistent
        annotations:
          description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only
            found {{ $value }} members of the {{$labels.job}} cluster.
          summary: A member of an Alertmanager cluster has not found all other cluster
            members.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
          < on (namespace,service) group_left
            count by (namespace,service) (max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m]))
        for: 15m
        labels:
          severity: warning
      - alert: AlertmanagerFailedToSendAlerts
        annotations:
          description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to
            send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration
            }}.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md
          summary: An Alertmanager instance failed to send notifications.
        expr: |
          (
            rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
          /
            rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: warning
      - alert: AlertmanagerClusterFailedToSendAlerts
        annotations:
          description: The minimum notification failure rate to {{ $labels.integration
            }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage
            }}.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md
          summary: All Alertmanager instances in a cluster failed to send notifications
            to a critical integration.
        expr: |
          min by (namespace,service, integration) (
            rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
          /
            rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: warning
      - alert: AlertmanagerConfigInconsistent
        annotations:
          description: Alertmanager instances within the {{$labels.job}} cluster have
            different configurations.
          summary: Alertmanager instances within the same cluster have different configurations.
        expr: |
          count by (namespace,service) (
            count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job=~"alertmanager-main|alertmanager-user-workload"})
          )
          != 1
        for: 20m
        labels:
          severity: warning
      - alert: AlertmanagerClusterDown
        annotations:
          description: '{{ $value | humanizePercentage }} of Alertmanager instances within
            the {{$labels.job}} cluster have been up for less than half of the last 5m.'
          summary: Half or more of the Alertmanager instances within the same cluster
            are down.
        expr: |
          (
            count by (namespace,service) (
              avg_over_time(up{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) < 0.5
            )
          /
            count by (namespace,service) (
              up{job=~"alertmanager-main|alertmanager-user-workload"}
            )
          )
          >= 0.5
        for: 5m
        labels:
          severity: warning
  openshift-monitoring-cluster-monitoring-operator-prometheus-rules-e392e114-8ec3-49da-a207-8120a57fb217.yaml: |
    groups:
    - name: openshift-general.rules
      rules:
      - alert: TargetDown
        annotations:
          description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
            }} targets in {{ $labels.namespace }} namespace have been unreachable for
            more than 15 minutes. This may be a symptom of network connectivity issues,
            down nodes, or failures within these components. Assess the health of the
            infrastructure and nodes running these targets and then contact support.'
          summary: Some targets were not reachable from the monitoring server for an extended
            period of time.
        expr: |
          100 * ((
            1 - sum   by (job, namespace, service) (up and on(namespace, pod) kube_pod_info) /
                count by (job, namespace, service) (up and on(namespace, pod) kube_pod_info)
          ) or (
            count by (job, namespace, service) (up == 0) /
            count by (job, namespace, service) (up)
          )) > 10
        for: 15m
        labels:
          severity: warning
    - name: openshift-kubernetes.rules
      rules:
      - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m])) BY
          (pod, namespace)
        record: pod:container_cpu_usage:sum
      - expr: sum(container_fs_usage_bytes{pod!=""}) BY (pod, namespace)
        record: pod:container_fs_usage_bytes:sum
      - expr: sum(container_memory_usage_bytes{container!=""}) BY (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m]))
          BY (namespace)
        record: namespace:container_cpu_usage:sum
      - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (cluster) / sum(machine_memory_bytes)
          BY (cluster)
        record: cluster:memory_usage:ratio
      - expr: sum(container_spec_cpu_shares{container="",pod!=""}) / 1000 / sum(machine_cpu_cores)
        record: cluster:container_spec_cpu_shares:ratio
      - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m])) /
          sum(machine_cpu_cores)
        record: cluster:container_cpu_usage:ratio
      - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and
          on(node) kube_node_role{role="master"})
        labels:
          label_node_role_kubernetes_io: master
          label_node_role_kubernetes_io_master: "true"
        record: cluster:master_nodes
      - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and
          on(node) kube_node_role{role="infra"})
        labels:
          label_node_role_kubernetes_io_infra: "true"
        record: cluster:infra_nodes
      - expr: max without(endpoint, instance, job, pod, service) (cluster:master_nodes
          and on(node) cluster:infra_nodes)
        labels:
          label_node_role_kubernetes_io_infra: "true"
          label_node_role_kubernetes_io_master: "true"
        record: cluster:master_infra_nodes
      - expr: cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node)
          cluster:infra_nodes or on (node) max without(endpoint, instance, job, pod, service)
          (kube_node_labels)
        record: cluster:nodes_roles
      - expr: kube_node_labels and on(node) (sum(label_replace(node_cpu_info, "node",
          "$1", "instance", "(.*)")) by (node, package, core) == 2)
        labels:
          label_node_hyperthread_enabled: "true"
        record: cluster:hyperthread_enabled_nodes
      - expr: count(sum(virt_platform) by (instance, type, system_manufacturer, system_product_name,
          baseboard_manufacturer, baseboard_product_name)) by (type, system_manufacturer,
          system_product_name, baseboard_manufacturer, baseboard_product_name)
        record: cluster:virt_platform_nodes:sum
      - expr: |
          sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (
            (
              cluster:master_nodes
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="cpu",unit="core"}
              )
            )
            or on(node) (
              max without(endpoint, instance, job, pod, service)
              (
                kube_node_labels
              ) * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="cpu",unit="core"}
              )
            )
          )
        record: cluster:capacity_cpu_cores:sum
      - expr: |
          clamp_max(
            label_replace(
              sum by(instance, package, core) (
                node_cpu_info{core!="",package!=""}
                or
                # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
                label_replace(label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
              ) > 1,
              "label_node_hyperthread_enabled",
              "true",
              "instance",
              "(.*)"
            ) or on (instance, package)
            label_replace(
              sum by(instance, package, core) (
                label_replace(node_cpu_info{core!="",package!=""}
                or
                # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
                label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
              ) <= 1,
              "label_node_hyperthread_enabled",
              "false",
              "instance",
              "(.*)"
            ),
            1
          )
        record: cluster:cpu_core_hyperthreading
      - expr: |
          topk by(node) (1, cluster:nodes_roles) * on (node)
            group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,
                         label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)
          label_replace( cluster:cpu_core_hyperthreading, "node", "$1", "instance", "(.*)" )
        record: cluster:cpu_core_node_labels
      - expr: count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled)
        record: cluster:capacity_cpu_cores_hyperthread_enabled:sum
      - expr: |
          sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
          (
            (
              cluster:master_nodes
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="memory",unit="byte"}
              )
            )
            or on(node)
            (
              max without(endpoint, instance, job, pod, service)
              (
                kube_node_labels
              )
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="memory",unit="byte"}
              )
            )
          )
        record: cluster:capacity_memory_bytes:sum
      - expr: sum(1 - rate(node_cpu_seconds_total{mode="idle"}[2m]) * on(namespace, pod)
          group_left(node) node_namespace_pod:kube_pod_info:{pod=~"node-exporter.+"})
        record: cluster:cpu_usage_cores:sum
      - expr: sum(node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"})
        record: cluster:memory_usage_bytes:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{namespace!~"openshift-.+",pod!="",container=""}[5m]))
        record: workload:cpu_usage_cores:sum
      - expr: cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum
        record: openshift:cpu_usage_cores:sum
      - expr: sum(container_memory_working_set_bytes{namespace!~"openshift-.+",pod!="",container=""})
        record: workload:memory_usage_bytes:sum
      - expr: cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum
        record: openshift:memory_usage_bytes:sum
      - expr: sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type,
          label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id)
        record: cluster:node_instance_type_count:sum
      - expr: |
          sum by(provisioner) (
            topk by (namespace, persistentvolumeclaim) (
              1, kube_persistentvolumeclaim_resource_requests_storage_bytes
            ) * on(namespace, persistentvolumeclaim) group_right()
            topk by(namespace, persistentvolumeclaim) (
              1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
            )
          )
        record: cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum
      - expr: (sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="",label_node_role_kubernetes_io_infra=""}
          or absent(__does_not_exist__)*0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="true"}
          or absent(__does_not_exist__)*0) * ((max(cluster_master_schedulable == 1)*0+1)
          or (absent(cluster_master_schedulable == 1)*0))))
        record: workload:capacity_physical_cpu_cores:sum
      - expr: min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
        record: cluster:usage:workload:capacity_physical_cpu_cores:min:5m
      - expr: max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
        record: cluster:usage:workload:capacity_physical_cpu_cores:max:5m
      - expr: |
          sum  by (provisioner) (
            topk by (namespace, persistentvolumeclaim) (
              1, kubelet_volume_stats_used_bytes
            ) * on (namespace,persistentvolumeclaim) group_right()
            topk by (namespace, persistentvolumeclaim) (
              1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
            )
          )
        record: cluster:kubelet_volume_stats_used_bytes:provisioner:sum
      - expr: sum by (instance) (apiserver_storage_objects)
        record: instance:etcd_object_counts:sum
      - expr: topk(500, max by(resource) (apiserver_storage_objects))
        record: cluster:usage:resources:sum
      - expr: count(count (kube_pod_restart_policy{type!="Always",namespace!~"openshift-.+"})
          by (namespace,pod))
        record: cluster:usage:pods:terminal:workload:sum
      - expr: sum(max(kubelet_containers_per_pod_count_sum) by (instance))
        record: cluster:usage:containers:sum
      - expr: count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled,
          label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        record: node_role_os_version_machine:cpu_capacity_cores:sum
      - expr: count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled, label_node_role_kubernetes_io)
        record: cluster:capacity_cpu_sockets_hyperthread_enabled:sum
      - expr: count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch,
          label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
          ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        record: node_role_os_version_machine:cpu_capacity_sockets:sum
      - expr: max(alertmanager_integrations{namespace="openshift-monitoring"})
        record: cluster:alertmanager_integrations:max
      - expr: sum by(plugin_name, volume_mode)(pv_collector_total_pv_count)
        record: cluster:kube_persistentvolume_plugin_type_counts:sum
      - expr: |
          sum(
            min by (node) (kube_node_status_condition{condition="Ready",status="true"})
              and
            max by (node) (kube_node_role{role="master"})
          ) == bool sum(kube_node_role{role="master"})
        record: cluster:control_plane:all_nodes_ready
      - alert: ClusterMonitoringOperatorReconciliationErrors
        annotations:
          description: Errors are occurring during reconciliation cycles. Inspect the
            cluster-monitoring-operator log for potential root causes.
          summary: Cluster Monitoring Operator is experiencing unexpected reconciliation
            errors.
        expr: max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m])
          == 0
        for: 1h
        labels:
          severity: warning
      - alert: AlertmanagerReceiversNotConfigured
        annotations:
          description: Alerts are not configured to be sent to a notification system,
            meaning that you may not be notified in a timely fashion when important failures
            occur. Check the OpenShift documentation to learn how to configure notifications
            with Alertmanager.
          summary: Receivers (notification integrations) are not configured on Alertmanager
        expr: cluster:alertmanager_integrations:max == 0
        for: 10m
        labels:
          namespace: openshift-monitoring
          severity: warning
      - alert: KubeDeploymentReplicasMismatch
        annotations:
          description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
            not matched the expected number of replicas for longer than 15 minutes. This
            indicates that cluster infrastructure is unable to start or restart the necessary
            components. This most often occurs when one or more nodes are down or partioned
            from the cluster, or a fault occurs on the node that prevents the workload
            from starting. In rare cases this may indicate a new version of a cluster
            component cannot start due to a bug or configuration error. Assess the pods
            for this deployment to verify they are running on healthy nodes and then contact
            support.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md
          summary: Deployment has not matched the expected number of replicas
        expr: |
          (((
            kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              >
            kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) and (
            changes(kube_deployment_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
              ==
            0
          )) * on() group_left cluster:control_plane:all_nodes_ready) > 0
        for: 15m
        labels:
          severity: warning
      - alert: MultipleContainersOOMKilled
        annotations:
          description: Multiple containers were out of memory killed within the past 15
            minutes. There are many potential causes of OOM errors, however issues on
            a specific node or containers breaching their limits is common.
          summary: Containers are being killed due to OOM
        expr: sum(max by(namespace, container, pod) (increase(kube_pod_container_status_restarts_total[12m]))
          and max by(namespace, container, pod) (kube_pod_container_status_last_terminated_reason{reason="OOMKilled"})
          == 1) > 5
        for: 15m
        labels:
          namespace: kube-system
          severity: info
      - expr: avg_over_time((((count((max by (node) (up{job="kubelet",metrics_path="/metrics"}
          == 1) and max by (node) (kube_node_status_condition{condition="Ready",status="true"}
          == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min
          by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])
        record: cluster:usage:kube_schedulable_node_ready_reachable:avg5m
      - expr: avg_over_time((count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}
          == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}))))[5m:1s])
        record: cluster:usage:kube_node_ready:avg5m
      - expr: (max without (condition,container,endpoint,instance,job,service) (((kube_pod_status_ready{condition="false"}
          == 1)*0 or (kube_pod_status_ready{condition="true"} == 1)) * on(pod,namespace)
          group_left() group by (pod,namespace) (kube_pod_status_phase{phase=~"Running|Unknown|Pending"}
          == 1)))
        record: kube_running_pod_ready
      - expr: avg(kube_running_pod_ready{namespace=~"openshift-.*"})
        record: cluster:usage:openshift:kube_running_pod_ready:avg
      - expr: avg(kube_running_pod_ready{namespace!~"openshift-.*"})
        record: cluster:usage:workload:kube_running_pod_ready:avg
      - alert: KubePodNotScheduled
        annotations:
          description: |-
            Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled for more than 30 minutes.
            Check the details of the pod with the following command:
            oc describe -n {{ $labels.namespace }} pod {{ $labels.pod }}
          summary: Pod cannot be scheduled.
        expr: last_over_time(kube_pod_status_unschedulable{namespace=~"(openshift-.*|kube-.*|default)"}[5m])
          == 1
        for: 30m
        labels:
          severity: warning
    - interval: 30s
      name: kubernetes-recurring.rules
      rules:
      - expr: sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds
          offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds
          offset 25s)*0))
        record: cluster:usage:workload:capacity_physical_cpu_core_seconds
    - name: openshift-ingress.rules
      rules:
      - expr: sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)
        record: code:cluster:ingress_http_request_count:rate5m:sum
      - expr: sum (rate(haproxy_frontend_bytes_in_total[5m]))
        record: cluster:usage:ingress_frontend_bytes_in:rate5m:sum
      - expr: sum (rate(haproxy_frontend_bytes_out_total[5m]))
        record: cluster:usage:ingress_frontend_bytes_out:rate5m:sum
      - expr: sum (haproxy_frontend_current_sessions)
        record: cluster:usage:ingress_frontend_connections:sum
      - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace!~"openshift-.*"}[5m])
          > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:workload:ingress_request_error:fraction5m
      - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:workload:ingress_request_total:irate5m
      - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace=~"openshift-.*"}[5m])
          > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:openshift:ingress_request_error:fraction5m
      - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:openshift:ingress_request_total:irate5m
      - expr: sum(ingress_controller_aws_nlb_active) or vector(0)
        record: cluster:ingress_controller_aws_nlb_active:sum
    - name: openshift-build.rules
      rules:
      - expr: sum by (strategy) (openshift_build_status_phase_total)
        record: openshift:build_by_strategy:sum
    - name: openshift-monitoring.rules
      rules:
      - expr: sum by (job,namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}))
        record: openshift:prometheus_tsdb_head_series:sum
      - expr: sum by(job,namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[2m])))
        record: openshift:prometheus_tsdb_head_samples_appended_total:sum
      - expr: sum by (namespace) (max without(instance) (container_memory_working_set_bytes{namespace=~"openshift-monitoring|openshift-user-workload-monitoring",
          container=""}))
        record: monitoring:container_memory_working_set_bytes:sum
      - expr: topk(3, sum by(namespace, job)(sum_over_time(scrape_series_added[1h])))
        record: namespace_job:scrape_series_added:topk3_sum1h
      - expr: topk(3, max by(namespace, job) (topk by(namespace,job) (1, scrape_samples_post_metric_relabeling)))
        record: namespace_job:scrape_samples_post_metric_relabeling:topk3
      - expr: sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace="openshift-monitoring",
          exported_service=~"alertmanager-main|prometheus-k8s"}[5m]))
        record: monitoring:haproxy_server_http_responses_total:sum
      - expr: max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job="kube-state-metrics",
          owner_kind="ReplicationController"},"replicationcontroller", "$1", "owner_name",
          "(.*)") * on(replicationcontroller, namespace) group_left(owner_name) topk by(replicationcontroller,
          namespace) (1, max by (replicationcontroller, namespace, owner_name) (kube_replicationcontroller_owner{job="kube-state-metrics"})),"workload",
          "$1", "owner_name", "(.*)"))
        labels:
          workload_type: deploymentconfig
        record: namespace_workload_pod:kube_pod_owner:relabel
    - name: openshift-etcd-telemetry.rules
      rules:
      - expr: sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job="etcd"})
        record: instance:etcd_mvcc_db_total_size_in_bytes:sum
      - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
        labels:
          quantile: "0.99"
        record: instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])))
        labels:
          quantile: "0.99"
        record: instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile
      - expr: sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job="etcd"})
        record: instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum
      - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])))
        labels:
          quantile: "0.99"
        record: instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile
    - name: openshift-sre.rules
      rules:
      - expr: sum(rate(apiserver_request_total{job="apiserver"}[10m])) BY (code)
        record: code:apiserver_request_total:rate:sum
    - name: openshift-vsphere.rules
      rules:
      - expr: sum by(version)(vsphere_vcenter_info)
        record: cluster:vsphere_vcenter_info:sum
      - expr: sum by(version)(vsphere_esxi_version_total)
        record: cluster:vsphere_esxi_version_total:sum
      - expr: sum by(hw_version)(vsphere_node_hw_version_total)
        record: cluster:vsphere_node_hw_version_total:sum
      - expr: max by(source)(vsphere_topology_tags)
        record: cluster:vsphere_topology_tags:max
      - expr: max by(scope)(vsphere_infrastructure_failure_domains)
        record: cluster:vsphere_infrastructure_failure_domains:max
    - name: general.rules
      rules:
      - alert: Watchdog
        annotations:
          description: |
            This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.
          summary: An alert that should always be firing to certify that Alertmanager
            is working properly.
        expr: vector(1)
        labels:
          namespace: openshift-monitoring
          severity: none
    - name: node-network
      rules:
      - alert: NodeNetworkInterfaceFlapping
        annotations:
          description: Network interface "{{ $labels.device }}" changing its up status
            often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Network interface is often changing its status
        expr: |
          changes(node_network_up{job="node-exporter",device!~"veth.+|tunbr"}[2m]) > 2
        for: 2m
        labels:
          severity: warning
    - name: kube-prometheus-node-recording.rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m]))
          BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance,
          cpu))
        record: cluster:node_cpu:ratio
    - name: kube-prometheus-general.rules
      rules:
      - expr: count without(instance, pod, node) (up == 1)
        record: count:up1
      - expr: count without(instance, pod, node) (up == 0)
        record: count:up0
  openshift-monitoring-kube-state-metrics-rules-2e5136d1-983b-4ab9-be62-c452eadd2b98.yaml: |
    groups:
    - name: kube-state-metrics
      rules:
      - alert: KubeStateMetricsListErrors
        annotations:
          description: kube-state-metrics is experiencing errors at an elevated rate in
            list operations. This is likely causing it to not be able to expose metrics
            about Kubernetes objects correctly or at all.
          summary: kube-state-metrics is experiencing errors in list operations.
        expr: |
          (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m]))
            /
          sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])))
          > 0.01
        for: 15m
        labels:
          severity: warning
      - alert: KubeStateMetricsWatchErrors
        annotations:
          description: kube-state-metrics is experiencing errors at an elevated rate in
            watch operations. This is likely causing it to not be able to expose metrics
            about Kubernetes objects correctly or at all.
          summary: kube-state-metrics is experiencing errors in watch operations.
        expr: |
          (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m]))
            /
          sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])))
          > 0.01
        for: 15m
        labels:
          severity: warning
  openshift-monitoring-kubernetes-monitoring-rules-35469c8a-01ac-4fee-be62-a98cd5075e32.yaml: |
    groups:
    - name: kubernetes-apps
      rules:
      - alert: KubePodCrashLooping
        annotations:
          description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
            }}) is in waiting state (reason: "CrashLoopBackOff").'
          summary: Pod is crash looping.
        expr: |
          max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m]) >= 1
        for: 15m
        labels:
          severity: warning
      - alert: KubePodNotReady
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
            state for longer than 15 minutes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md
          summary: Pod has been in a non-ready state for more than 15 minutes.
        expr: |
          sum by (namespace, pod, cluster) (
            max by(namespace, pod, cluster) (
              kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default)", job="kube-state-metrics", phase=~"Pending|Unknown"}
              unless ignoring(phase) (kube_pod_status_unschedulable{job="kube-state-metrics"} == 1)
            ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
              1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
            )
          ) > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeDeploymentGenerationMismatch
        annotations:
          description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
            }} does not match, this indicates that the Deployment has failed but has not
            been rolled back.
          summary: Deployment generation mismatch due to possible roll-back
        expr: |
          kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
      - alert: KubeStatefulSetReplicasMismatch
        annotations:
          description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
            not matched the expected number of replicas for longer than 15 minutes.
          summary: Deployment has not matched the expected number of replicas.
        expr: |
          (
            kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) and (
            changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[10m])
              ==
            0
          )
        for: 15m
        labels:
          severity: warning
      - alert: KubeStatefulSetGenerationMismatch
        annotations:
          description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
            }} does not match, this indicates that the StatefulSet has failed but has
            not been rolled back.
          summary: StatefulSet generation mismatch due to possible roll-back
        expr: |
          kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
      - alert: KubeStatefulSetUpdateNotRolledOut
        annotations:
          description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
            has not been rolled out.
          summary: StatefulSet update has not been rolled out.
        expr: |
          (
            max without (revision) (
              kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                unless
              kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            )
              *
            (
              kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            )
          )  and (
            changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
              ==
            0
          )
        for: 15m
        labels:
          severity: warning
      - alert: KubeDaemonSetRolloutStuck
        annotations:
          description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not
            finished or progressed for at least 30 minutes.
          summary: DaemonSet rollout is stuck.
        expr: |
          (
            (
              kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ) or (
              kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              0
            ) or (
              kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ) or (
              kube_daemonset_status_number_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            )
          ) and (
            changes(kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
              ==
            0
          )
        for: 30m
        labels:
          severity: warning
      - alert: KubeContainerWaiting
        annotations:
          description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container
            {{ $labels.container}} has been in waiting state for longer than 1 hour.
          summary: Pod container waiting longer than 1 hour
        expr: |
          sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}) > 0
        for: 1h
        labels:
          severity: warning
      - alert: KubeDaemonSetNotScheduled
        annotations:
          description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are not scheduled.'
          summary: DaemonSet pods are not scheduled.
        expr: |
          kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            -
          kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: warning
      - alert: KubeDaemonSetMisScheduled
        annotations:
          description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are running where they are not supposed to run.'
          summary: DaemonSet pods are misscheduled.
        expr: |
          kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeJobNotCompleted
        annotations:
          description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
            than {{ "43200" | humanizeDuration }} to complete.
          summary: Job did not complete in time
        expr: |
          time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            and
          kube_job_status_active{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0) > 43200
        labels:
          severity: warning
      - alert: KubeJobFailed
        annotations:
          description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
            Removing failed job after investigation should clear this alert.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md
          summary: Job failed to complete.
        expr: |
          kube_job_failed{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}  > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeHpaReplicasMismatch
        annotations:
          description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
            has not matched the desired number of replicas for longer than 15 minutes.
          summary: HPA has not matched desired number of replicas.
        expr: |
          (kube_horizontalpodautoscaler_status_desired_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
            and
          (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            >
          kube_horizontalpodautoscaler_spec_min_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
            and
          (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            <
          kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
            and
          changes(kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[15m]) == 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeHpaMaxedOut
        annotations:
          description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
            has been running at max replicas for longer than 15 minutes.
          summary: HPA is running at max replicas
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ==
          kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-resources
      rules:
      - alert: KubeCPUOvercommit
        annotations:
          description: Cluster has overcommitted CPU resource requests for Pods by {{
            $value }} CPU shares and cannot tolerate node failure.
          summary: Cluster has overcommitted CPU resource requests.
        expr: |
          sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) - (sum(kube_node_status_allocatable{resource="cpu"}) - max(kube_node_status_allocatable{resource="cpu"})) > 0
          and
          (sum(kube_node_status_allocatable{resource="cpu"}) - max(kube_node_status_allocatable{resource="cpu"})) > 0
        for: 10m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeMemoryOvercommit
        annotations:
          description: Cluster has overcommitted memory resource requests for Pods by
            {{ $value | humanize }} bytes and cannot tolerate node failure.
          summary: Cluster has overcommitted memory resource requests.
        expr: |
          sum(namespace_memory:kube_pod_container_resource_requests:sum{}) - (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
          and
          (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
        for: 10m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeCPUQuotaOvercommit
        annotations:
          description: Cluster has overcommitted CPU resource requests for Namespaces.
          summary: Cluster has overcommitted CPU resource requests.
        expr: |
          sum(min without(resource) (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"}))
            /
          sum(kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"})
            > 1.5
        for: 5m
        labels:
          severity: warning
      - alert: KubeMemoryQuotaOvercommit
        annotations:
          description: Cluster has overcommitted memory resource requests for Namespaces.
          summary: Cluster has overcommitted memory resource requests.
        expr: |
          sum(min without(resource) (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"}))
            /
          sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"})
            > 1.5
        for: 5m
        labels:
          severity: warning
      - alert: KubeQuotaAlmostFull
        annotations:
          description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
            }} of its {{ $labels.resource }} quota.
          summary: Namespace quota is going to be full.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
            > 0.9 < 1
        for: 15m
        labels:
          severity: info
      - alert: KubeQuotaFullyUsed
        annotations:
          description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
            }} of its {{ $labels.resource }} quota.
          summary: Namespace quota is fully used.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
            == 1
        for: 15m
        labels:
          severity: info
      - alert: KubeQuotaExceeded
        annotations:
          description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
            }} of its {{ $labels.resource }} quota.
          summary: Namespace quota has exceeded the limits.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
            > 1
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-storage
      rules:
      - alert: KubePersistentVolumeFillingUp
        annotations:
          description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
            }} free.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
          summary: PersistentVolume is filling up.
        expr: |
          (
            kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
              /
            kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
          ) < 0.03
          and
          kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
        for: 1m
        labels:
          severity: critical
      - alert: KubePersistentVolumeFillingUp
        annotations:
          description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is expected to fill up within four
            days. Currently {{ $value | humanizePercentage }} is available.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
          summary: PersistentVolume is filling up.
        expr: |
          (
            kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
              /
            kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
          ) < 0.15
          and
          kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
          and
          predict_linear(kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
        for: 1h
        labels:
          severity: warning
      - alert: KubePersistentVolumeInodesFillingUp
        annotations:
          description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage
            }} free inodes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
          summary: PersistentVolumeInodes are filling up.
        expr: |
          (
            kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
              /
            kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
          ) < 0.03
          and
          kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
        for: 1m
        labels:
          severity: critical
      - alert: KubePersistentVolumeInodesFillingUp
        annotations:
          description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within
            four days. Currently {{ $value | humanizePercentage }} of its inodes are free.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
          summary: PersistentVolumeInodes are filling up.
        expr: |
          (
            kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
              /
            kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
          ) < 0.15
          and
          kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
          and
          predict_linear(kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
          unless on(namespace, persistentvolumeclaim)
          kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
        for: 1h
        labels:
          severity: warning
      - alert: KubePersistentVolumeErrors
        annotations:
          description: The persistent volume {{ $labels.persistentvolume }} has status
            {{ $labels.phase }}.
          summary: PersistentVolume is having issues with provisioning.
        expr: |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
        for: 5m
        labels:
          severity: warning
    - name: kubernetes-system
      rules:
      - alert: KubeClientErrors
        annotations:
          description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}' is experiencing {{ $value | humanizePercentage }} errors.'
          summary: Kubernetes API server client is experiencing errors.
        expr: |
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (cluster, instance, job, namespace)
            /
          sum(rate(rest_client_requests_total[5m])) by (cluster, instance, job, namespace))
          > 0.01
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-system-apiserver
      rules:
      - alert: KubeAggregatedAPIErrors
        annotations:
          description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace
            }} has reported errors. It has appeared unavailable {{ $value | humanize }}
            times averaged over the past 10m.
          summary: Kubernetes aggregated API has reported errors.
        expr: |
          sum by(name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total[10m])) > 4
        labels:
          severity: warning
      - alert: KubeAggregatedAPIDown
        annotations:
          description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace
            }} has been only {{ $value | humanize }}% available over the last 10m.
          summary: Kubernetes aggregated API is down.
        expr: |
          (1 - max by(name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85
        for: 15m
        labels:
          severity: warning
      - alert: KubeAPIDown
        annotations:
          description: KubeAPI has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeAPIDown.md
          summary: Target disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="apiserver"} == 1)
        for: 15m
        labels:
          severity: critical
      - alert: KubeAPITerminatedRequests
        annotations:
          description: The kubernetes apiserver has terminated {{ $value | humanizePercentage
            }} of its incoming requests.
          summary: The kubernetes apiserver has terminated {{ $value | humanizePercentage
            }} of its incoming requests.
        expr: |
          sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m]))  / (  sum(rate(apiserver_request_total{job="apiserver"}[10m])) + sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m])) ) > 0.20
        for: 5m
        labels:
          severity: warning
    - name: kubernetes-system-kubelet
      rules:
      - alert: KubeNodeNotReady
        annotations:
          description: '{{ $labels.node }} has been unready for more than 15 minutes.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md
          summary: Node is not ready.
        expr: |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeNodeUnreachable
        annotations:
          description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
          summary: Node is unreachable.
        expr: |
          (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
        for: 15m
        labels:
          severity: warning
      - alert: KubeletTooManyPods
        annotations:
          description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
            }} of its Pod capacity.
          summary: Kubelet is running at capacity.
        expr: |
          count by(cluster, node) (
            (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
          )
          /
          max by(cluster, node) (
            kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
          ) > 0.95
        for: 15m
        labels:
          severity: info
      - alert: KubeNodeReadinessFlapping
        annotations:
          description: The readiness status of node {{ $labels.node }} has changed {{
            $value }} times in the last 15 minutes.
          summary: Node readiness status is flapping.
        expr: |
          sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (cluster, node) > 2
        for: 15m
        labels:
          severity: warning
      - alert: KubeletPlegDurationHigh
        annotations:
          description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
            duration of {{ $value }} seconds on node {{ $labels.node }}.
          summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
        expr: |
          node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
        for: 5m
        labels:
          severity: warning
      - alert: KubeletPodStartUpLatencyHigh
        annotations:
          description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds
            on node {{ $labels.node }}.
          summary: Kubelet Pod startup latency is too high.
        expr: |
          histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
        for: 15m
        labels:
          severity: warning
      - alert: KubeletClientCertificateRenewalErrors
        annotations:
          description: Kubelet on node {{ $labels.node }} has failed to renew its client
            certificate ({{ $value | humanize }} errors in the last 5 minutes).
          summary: Kubelet has failed to renew its client certificate.
        expr: |
          increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeletServerCertificateRenewalErrors
        annotations:
          description: Kubelet on node {{ $labels.node }} has failed to renew its server
            certificate ({{ $value | humanize }} errors in the last 5 minutes).
          summary: Kubelet has failed to renew its server certificate.
        expr: |
          increase(kubelet_server_expiration_renew_errors[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeletDown
        annotations:
          description: Kubelet has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md
          summary: Target disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kubelet", metrics_path="/metrics"} == 1)
        for: 15m
        labels:
          namespace: kube-system
          severity: critical
    - name: k8s.rules
      rules:
      - expr: |
          sum by (cluster, namespace, pod, container) (
            irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
          ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
            1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
      - expr: |
          container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_working_set_bytes
      - expr: |
          container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_rss
      - expr: |
          container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_cache
      - expr: |
          container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_swap
      - expr: |
          kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
            (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
          )
        record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_memory:kube_pod_container_resource_requests:sum
      - expr: |
          kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
            (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
          )
        record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_cpu:kube_pod_container_resource_requests:sum
      - expr: |
          kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
            (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
          )
        record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_memory:kube_pod_container_resource_limits:sum
      - expr: |
          kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
           (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
           )
        record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_cpu:kube_pod_container_resource_limits:sum
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                "replicaset", "$1", "owner_name", "(.*)"
              ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                1, max by (replicaset, namespace, owner_name) (
                  kube_replicaset_owner{job="kube-state-metrics"}
                )
              ),
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: deployment
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: daemonset
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: statefulset
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: job
        record: namespace_workload_pod:kube_pod_owner:relabel
    - name: kube-scheduler.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - name: node.rules
      rules:
      - expr: |
          topk by(cluster, namespace, pod) (1,
            max by (cluster, node, namespace, pod) (
              label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
          ))
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |
          sum(
            node_memory_MemAvailable_bytes{job="node-exporter"} or
            (
              node_memory_Buffers_bytes{job="node-exporter"} +
              node_memory_Cached_bytes{job="node-exporter"} +
              node_memory_MemFree_bytes{job="node-exporter"} +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          ) by (cluster)
        record: :node_memory_MemAvailable_bytes:sum
      - expr: |
          avg by (cluster, node) (
            sum without (mode) (
              rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
            )
          )
        record: node:node_cpu_utilization:ratio_rate5m
      - expr: |
          avg by (cluster) (
            node:node_cpu_utilization:ratio_rate5m
          )
        record: cluster:node_cpu:ratio_rate5m
    - name: kubelet.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: "0.99"
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: "0.9"
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: "0.5"
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
  openshift-monitoring-node-exporter-rules-63cedfa4-184a-49c6-a563-aa493cecd8f2.yaml: |
    groups:
    - name: node-exporter
      rules:
      - alert: NodeFilesystemSpaceFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left and is filling up.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
          summary: Filesystem is predicted to run out of space within the next 24 hours.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 15
          and
            predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemSpaceFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left and is filling up fast.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
          summary: Filesystem is predicted to run out of space within the next 4 hours.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 10
          and
            predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemAlmostOutOfSpace
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
          summary: Filesystem has less than 5% space left.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 30m
        labels:
          severity: warning
      - alert: NodeFilesystemAlmostOutOfSpace
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
          summary: Filesystem has less than 3% space left.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 30m
        labels:
          severity: critical
      - alert: NodeFilesystemFilesFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left and is filling up.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
          summary: Filesystem is predicted to run out of inodes within the next 24 hours.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 40
          and
            predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemFilesFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
          summary: Filesystem is predicted to run out of inodes within the next 4 hours.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 20
          and
            predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemAlmostOutOfFiles
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
          summary: Filesystem has less than 5% inodes left.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemAlmostOutOfFiles
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
          summary: Filesystem has less than 3% inodes left.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeNetworkReceiveErrs
        annotations:
          description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
            {{ printf "%.0f" $value }} receive errors in the last two minutes.'
          summary: Network interface is reporting many receive errors.
        expr: |
          rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
        for: 1h
        labels:
          severity: warning
      - alert: NodeNetworkTransmitErrs
        annotations:
          description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
            {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
          summary: Network interface is reporting many transmit errors.
        expr: |
          rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
        for: 1h
        labels:
          severity: warning
      - alert: NodeHighNumberConntrackEntriesUsed
        annotations:
          description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
          summary: Number of conntrack are getting close to the limit.
        expr: |
          (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
        labels:
          severity: warning
      - alert: NodeTextFileCollectorScrapeError
        annotations:
          description: Node Exporter text file collector failed to scrape.
          summary: Node Exporter text file collector failed to scrape.
        expr: |
          node_textfile_scrape_error{job="node-exporter"} == 1
        labels:
          severity: warning
      - alert: NodeClockSkewDetected
        annotations:
          description: Clock on {{ $labels.instance }} is out of sync by more than 300s.
            Ensure NTP is configured correctly on this host.
          summary: Clock skew detected.
        expr: |
          (
            node_timex_offset_seconds{job="node-exporter"} > 0.05
          and
            deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
          )
          or
          (
            node_timex_offset_seconds{job="node-exporter"} < -0.05
          and
            deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
          )
        for: 10m
        labels:
          severity: warning
      - alert: NodeClockNotSynchronising
        annotations:
          description: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP
            is configured on this host.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md
          summary: Clock not synchronising.
        expr: |
          min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
          and
          node_timex_maxerror_seconds{job="node-exporter"} >= 16
        for: 10m
        labels:
          severity: critical
      - alert: NodeRAIDDegraded
        annotations:
          description: RAID array '{{ $labels.device }}' on {{ $labels.instance }} is
            in degraded state due to one or more disks failures. Number of spare drives
            is insufficient to fix issue automatically.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md
          summary: RAID Array is degraded
        expr: |
          node_md_disks_required{job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}) > 0
        for: 15m
        labels:
          severity: critical
      - alert: NodeRAIDDiskFailure
        annotations:
          description: At least one device in RAID array on {{ $labels.instance }} failed.
            Array '{{ $labels.device }}' needs attention and possibly a disk swap.
          summary: Failed device in RAID array
        expr: |
          node_md_disks{state="failed",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} > 0
        labels:
          severity: warning
      - alert: NodeFileDescriptorLimit
        annotations:
          description: File descriptors limit at {{ $labels.instance }} is currently at
            {{ printf "%.2f" $value }}%.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
          summary: Kernel is predicted to exhaust file descriptors limit soon.
        expr: |
          (
            node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
          )
        for: 15m
        labels:
          severity: warning
      - alert: NodeFileDescriptorLimit
        annotations:
          description: File descriptors limit at {{ $labels.instance }} is currently at
            {{ printf "%.2f" $value }}%.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
          summary: Kernel is predicted to exhaust file descriptors limit soon.
        expr: |
          (
            node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
          )
        for: 15m
        labels:
          severity: critical
    - name: node-exporter.rules
      rules:
      - expr: |
          count without (cpu, mode) (
            node_cpu_seconds_total{job="node-exporter",mode="idle"}
          )
        record: instance:node_num_cpu:sum
      - expr: |
          1 - avg without (cpu) (
            sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[1m]))
          )
        record: instance:node_cpu_utilisation:rate1m
      - expr: |
          (
            node_load1{job="node-exporter"}
          /
            instance:node_num_cpu:sum{job="node-exporter"}
          )
        record: instance:node_load1_per_cpu:ratio
      - expr: |
          1 - (
            (
              node_memory_MemAvailable_bytes{job="node-exporter"}
              or
              (
                node_memory_Buffers_bytes{job="node-exporter"}
                +
                node_memory_Cached_bytes{job="node-exporter"}
                +
                node_memory_MemFree_bytes{job="node-exporter"}
                +
                node_memory_Slab_bytes{job="node-exporter"}
              )
            )
          /
            node_memory_MemTotal_bytes{job="node-exporter"}
          )
        record: instance:node_memory_utilisation:ratio
      - expr: |
          rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
        record: instance:node_vmstat_pgmajfault:rate1m
      - expr: |
          rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_seconds:rate1m
      - expr: |
          rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_weighted_seconds:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_bytes_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_bytes_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_drop_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_drop_excluding_lo:rate1m
  openshift-monitoring-prometheus-k8s-prometheus-rules-f3896e7a-06f0-4524-b501-13e4bf6673eb.yaml: |
    groups:
    - name: prometheus
      rules:
      - alert: PrometheusBadConfig
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
            reload its configuration.
          summary: Failed Prometheus configuration reload.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(prometheus_config_last_reload_successful{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) == 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
            is running full.
          summary: Prometheus alert notification queue predicted to run full in less than
            30m.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            predict_linear(prometheus_notifications_queue_length{job=~"prometheus-k8s|prometheus-user-workload"}[5m], 60 * 30)
          >
            min_over_time(prometheus_notifications_queue_capacity{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
        annotations:
          description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus
            {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
          summary: Prometheus has encountered more than 1% errors sending alerts to a
            specific Alertmanager.
        expr: |
          (
            rate(prometheus_notifications_errors_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          /
            rate(prometheus_notifications_sent_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
            to any Alertmanagers.
          summary: Prometheus is not connected to any Alertmanagers.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(prometheus_notifications_alertmanagers_discovered{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) < 1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusTSDBReloadsFailing
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
            | humanize}} reload failures over the last 3h.
          summary: Prometheus has issues reloading blocks from disk.
        expr: |
          increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
            | humanize}} compaction failures over the last 3h.
          summary: Prometheus has issues compacting blocks.
        expr: |
          increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusNotIngestingSamples
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
            samples.
          summary: Prometheus is not ingesting samples.
        expr: |
          (
            rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) <= 0
          and
            (
              sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
            or
              sum without(rule_group) (prometheus_rule_group_rules{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
            )
          )
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusDuplicateTimestamps
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{
            printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
          summary: Prometheus is dropping samples with duplicate timestamps.
        expr: |
          rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 1h
        labels:
          severity: warning
      - alert: PrometheusOutOfOrderTimestamps
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{
            printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
          summary: Prometheus drops samples with out-of-order timestamps.
        expr: |
          rate(prometheus_target_scrapes_sample_out_of_order_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 1h
        labels:
          severity: warning
      - alert: PrometheusRemoteStorageFailures
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send
            {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{
            $labels.url }}
          summary: Prometheus fails to send samples to remote storage.
        expr: |
          (
            (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
          /
            (
              (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
            +
              (rate(prometheus_remote_storage_succeeded_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
            )
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusRemoteWriteBehind
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is
            {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url
            }}.
          summary: Prometheus remote write is behind.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          - ignoring(remote_name, url) group_right
            max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
          > 120
        for: 15m
        labels:
          severity: info
      - alert: PrometheusRemoteWriteDesiredShards
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired
            shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{
            $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job=~"prometheus-k8s|prometheus-user-workload"}`
            $labels.instance | query | first | value }}.
          summary: Prometheus remote write desired shards calculation wants to run more
            than configured max shards.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            max_over_time(prometheus_remote_storage_shards_desired{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          >
            max_over_time(prometheus_remote_storage_shards_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusRuleFailures
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
            evaluate {{ printf "%.0f" $value }} rules in the last 5m.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRuleFailures.md
          summary: Prometheus is failing rule evaluations.
        expr: |
          increase(prometheus_rule_evaluation_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusMissingRuleEvaluations
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
            printf "%.0f" $value }} rule group evaluations in the last 5m.
          summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
        expr: |
          increase(prometheus_rule_group_iterations_missed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusTargetLimitHit
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{
            printf "%.0f" $value }} targets because the number of targets exceeded the
            configured target_limit.
          summary: Prometheus has dropped targets because some scrape configs have exceeded
            the targets limit.
        expr: |
          increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusLabelLimitHit
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{
            printf "%.0f" $value }} targets because some samples exceeded the configured
            label_limit, label_name_length_limit or label_value_length_limit.
          summary: Prometheus has dropped targets because some scrape configs have exceeded
            the labels limit.
        expr: |
          increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusScrapeBodySizeLimitHit
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{
            printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded
            the configured body_size_limit.
          summary: Prometheus has dropped some targets that exceeded body size limit.
        expr: |
          increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusScrapeSampleLimitHit
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{
            printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded
            the configured sample_limit.
          summary: Prometheus has failed scrapes that have exceeded the configured sample
            limit.
        expr: |
          increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusTargetSyncFailure
        annotations:
          description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}}
            have failed to sync because invalid configuration was supplied.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md
          summary: Prometheus has failed to sync targets.
        expr: |
          increase(prometheus_target_sync_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[30m]) > 0
        for: 5m
        labels:
          severity: critical
      - alert: PrometheusHighQueryLoad
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has
            less than 20% available capacity in its query engine for the last 15 minutes.
          summary: Prometheus is reaching its maximum capacity serving concurrent requests.
        expr: |
          avg_over_time(prometheus_engine_queries{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0.8
        for: 15m
        labels:
          severity: warning
  openshift-monitoring-prometheus-k8s-thanos-sidecar-rules-88a5d2b5-2c83-4cdb-817f-c756d68bdc6e.yaml: |
    groups:
    - name: thanos-sidecar
      rules:
      - alert: ThanosSidecarBucketOperationsFailed
        annotations:
          description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} bucket
            operations are failing
          summary: Thanos Sidecar bucket operations are failing
        expr: |
          sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"}[5m])) > 0
        for: 1h
        labels:
          severity: warning
      - alert: ThanosSidecarNoConnectionToStartedPrometheus
        annotations:
          description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} is
            unhealthy.
          summary: Thanos Sidecar cannot access Prometheus, even though Prometheus seems
            healthy and has reloaded WAL.
        expr: |
          thanos_sidecar_prometheus_up{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"} == 0
          AND on (namespace, pod)
          prometheus_tsdb_data_replay_duration_seconds != 0
        for: 1h
        labels:
          severity: warning
  openshift-monitoring-prometheus-operator-rules-d6eb23ae-a271-48c9-ace5-ca9daa2be1c0.yaml: |
    groups:
    - name: prometheus-operator
      rules:
      - alert: PrometheusOperatorListErrors
        annotations:
          description: Errors while performing List operations in controller {{$labels.controller}}
            in {{$labels.namespace}} namespace.
          summary: Errors while performing list operations in controller.
        expr: |
          (sum by (controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m])) / sum by (controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m]))) > 0.4
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusOperatorWatchErrors
        annotations:
          description: Errors while performing watch operations in controller {{$labels.controller}}
            in {{$labels.namespace}} namespace.
          summary: Errors while performing watch operations in controller.
        expr: |
          (sum by (controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m])) / sum by (controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.4
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusOperatorSyncFailed
        annotations:
          description: Controller {{ $labels.controller }} in {{ $labels.namespace }}
            namespace fails to reconcile {{ $value }} objects.
          summary: Last controller reconciliation failed
        expr: |
          min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorReconcileErrors
        annotations:
          description: '{{ $value | humanizePercentage }} of reconciling operations failed
            for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
          summary: Errors while reconciling controller.
        expr: |
          (sum by (controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorNodeLookupErrors
        annotations:
          description: Errors while reconciling Prometheus in {{ $labels.namespace }}
            Namespace.
          summary: Errors while reconciling Prometheus.
        expr: |
          rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorNotReady
        annotations:
          description: Prometheus operator in {{ $labels.namespace }} namespace isn't
            ready to reconcile {{ $labels.controller }} resources.
          summary: Prometheus operator not ready
        expr: |
          min by (controller,namespace) (max_over_time(prometheus_operator_ready{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) == 0)
        for: 5m
        labels:
          severity: warning
      - alert: PrometheusOperatorRejectedResources
        annotations:
          description: Prometheus operator in {{ $labels.namespace }} namespace rejected
            {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }}
            resources.
          summary: Resources rejected by Prometheus operator
        expr: |
          min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
    - name: config-reloaders
      rules:
      - alert: ConfigReloaderSidecarErrors
        annotations:
          description: |-
            Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
            As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
          summary: config-reloader sidecar has not had a successful reload for 10m
        expr: |
          max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
        for: 10m
        labels:
          severity: warning
  openshift-monitoring-telemetry-611ae6c7-3fe7-4c1c-ad21-654fbe81578b.yaml: |
    groups:
    - name: telemeter.rules
      rules:
      - expr: max(federate_samples - federate_filtered_samples)
        record: cluster:telemetry_selected_series:count
      - alert: TelemeterClientFailures
        annotations:
          description: |-
            The telemeter client in namespace {{ $labels.namespace }} fails {{ $value | humanize }} of the requests to the telemeter service.
            Check the logs of the telemeter-client pod with the following command:
            oc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client
            If the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.
          summary: Telemeter client fails to send metrics
        expr: |
          sum by (namespace) (
            rate(federate_requests_failed_total{job="telemeter-client"}[15m])
          ) /
          sum by (namespace) (
            rate(federate_requests_total{job="telemeter-client"}[15m])
          ) > 0.2
        for: 1h
        labels:
          severity: warning
  openshift-multus-prometheus-k8s-rules-7b3c0e43-891b-49e9-8138-ab95706596ed.yaml: |
    groups:
    - name: multus-admission-controller-monitor-service.rules
      rules:
      - expr: |
          max  (network_attachment_definition_enabled_instance_up) by (networks)
        record: cluster:network_attachment_definition_enabled_instance_up:max
      - expr: |
          max  (network_attachment_definition_instances) by (networks)
        record: cluster:network_attachment_definition_instances:max
  openshift-operator-lifecycle-manager-olm-alert-rules-89a6a32e-7e2e-45f4-bcd7-ef677783cb6a.yaml: |
    groups:
    - name: olm.csv_abnormal.rules
      rules:
      - alert: CsvAbnormalFailedOver2Min
        annotations:
          description: Fires whenever a CSV has been in the failed phase for more than
            2 minutes.
          message: Failed to install Operator {{ $labels.name }} version {{ $labels.version
            }}. Reason-{{ $labels.reason }}
          summary: CSV failed for over 2 minutes
        expr: csv_abnormal{phase=~"^Failed$"}
        for: 2m
        labels:
          namespace: '{{ $labels.namespace }}'
          severity: warning
      - alert: CsvAbnormalOver30Min
        annotations:
          description: Fires whenever a CSV is in the Replacing, Pending, Deleting, or
            Unkown phase for more than 30 minutes.
          message: Failed to install Operator {{ $labels.name }} version {{ $labels.version
            }}. Phase-{{ $labels.phase }} Reason-{{ $labels.reason }}
          summary: CSV abnormal for over 30 minutes
        expr: csv_abnormal{phase=~"(^Replacing$|^Pending$|^Deleting$|^Unknown$)"}
        for: 30m
        labels:
          namespace: '{{ $labels.namespace }}'
          severity: warning
    - name: olm.installplan.rules
      rules:
      - alert: InstallPlanStepAppliedWithWarnings
        annotations:
          description: Fires whenever the API server returns a warning when attempting
            to modify an operator.
          message: The API server returned a warning during installation or upgrade of
            an operator. An Event with reason "AppliedWithWarnings" has been created with
            complete details, including a reference to the InstallPlan step that generated
            the warning.
          summary: API returned a warning when modifying an operator
        expr: sum(increase(installplan_warnings_total[5m])) > 0
        labels:
          severity: warning
  openshift-ovn-kubernetes-master-rules-8bcc512d-2364-455d-895d-b47540f89a48.yaml: |
    groups:
    - name: cluster-network-operator-master.rules
      rules:
      - expr: max(ovnkube_master_egress_routing_via_host)
        record: cluster:ovnkube_master_egress_routing_via_host:max
      - expr: abs(count(ovn_db_cluster_server_status{db_name="OVN_Northbound", server_status="cluster
          member"}) - 1)
        record: cluster:ovn_db_nbdb_not_cluster_member:abs
      - expr: abs(count(ovn_db_cluster_server_status{db_name="OVN_Southbound", server_status="cluster
          member"}) - 1)
        record: cluster:ovn_db_sbdb_not_cluster_member:abs
      - expr: abs(sum(ovn_db_cluster_inbound_connections_total{db_name="OVN_Northbound"})
          - (1 * (1-1)))
        record: cluster:ovn_db_nbdb_missing_inbound_connections:abs
      - expr: abs(sum(ovn_db_cluster_inbound_connections_total{db_name="OVN_Southbound"})
          - (1 * (1-1)))
        record: cluster:ovn_db_sbdb_missing_inbound_connections:abs
      - expr: abs(sum(ovn_db_cluster_outbound_connections_total{db_name="OVN_Northbound"})
          - (1 * (1-1)))
        record: cluster:ovn_db_nbdb_missing_outbound_connections:abs
      - expr: abs(sum(ovn_db_cluster_outbound_connections_total{db_name="OVN_Southbound"})
          - (1 * (1-1)))
        record: cluster:ovn_db_sbdb_missing_outbound_connections:abs
      - alert: V4SubnetAllocationThresholdExceeded
        annotations:
          description: More than 80% of IPv4 subnets are used. Insufficient IPv4 subnets
            could degrade provisioning of workloads.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/V4SubnetAllocationThresholdExceeded.md
          summary: More than 80% of v4 subnets available to assign to the nodes are allocated.
            Current v4 subnet allocation percentage is {{ $value | humanizePercentage
            }}.
        expr: ovnkube_clustermanager_allocated_v4_host_subnets / ovnkube_clustermanager_num_v4_host_subnets
          > 0.8
        for: 10m
        labels:
          severity: warning
      - alert: V6SubnetAllocationThresholdExceeded
        annotations:
          description: More than 80% of IPv6 subnets are used. Insufficient IPv6 subnets
            could degrade provisioning of workloads.
          summary: More than 80% of the v6 subnets available to assign to the nodes are
            allocated. Current v6 subnet allocation percentage is {{ $value | humanizePercentage
            }}.
        expr: ovnkube_clustermanager_allocated_v6_host_subnets / ovnkube_clustermanager_num_v6_host_subnets
          > 0.8
        for: 10m
        labels:
          severity: warning
      - alert: NoRunningOvnMaster
        annotations:
          description: |
            Networking control plane is degraded. Networking configuration updates applied to the cluster will not be
            implemented while there are no OVN Kubernetes pods.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NoRunningOvnMaster.md
          summary: There is no running ovn-kubernetes master.
        expr: |
          absent(up{job="ovnkube-master", namespace="openshift-ovn-kubernetes"} == 1)
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: critical
      - alert: NoOvnMasterLeader
        annotations:
          description: |
            Networking control plane is degraded. Networking configuration updates applied to the cluster will not be
            implemented while there is no OVN Kubernetes leader. Existing workloads should continue to have connectivity.
            OVN-Kubernetes control plane is not functional.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NoOvnMasterLeader.md
          summary: There is no ovn-kubernetes master leader.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max by (namespace) (max_over_time(ovnkube_master_leader[5m])) == 0
        for: 5m
        labels:
          severity: critical
      - alert: NorthboundStale
        annotations:
          description: |
            Networking control plane is degraded. Networking configuration updates applied to the cluster will not be
            implemented. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane and/or
            OVN northbound database may not be functional.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NorthboundStaleAlert.md
          summary: ovn-kubernetes has not written anything to the northbound database
            for too long.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          time() - max_over_time(ovnkube_master_nb_e2e_timestamp[5m]) > 120
        for: 10m
        labels:
          severity: warning
      - alert: SouthboundStale
        annotations:
          description: |
            Networking control plane is degraded. Networking configuration updates may not be applied to the cluster or
            taking a long time to apply. This usually means there is a large load on OVN component 'northd' or it is not
            functioning.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/SouthboundStaleAlert.md
          summary: ovn-northd has not successfully synced any changes to the southbound
            DB for too long.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(ovnkube_master_nb_e2e_timestamp[5m]) - max_over_time(ovnkube_master_sb_e2e_timestamp[5m]) > 120
        for: 10m
        labels:
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseClusterIDError
        annotations:
          description: More than one OVN northbound database cluster ID indicates degraded
            OVN database high availability and possible database split brain.
          summary: Multiple OVN northbound database cluster IDs exist.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(count(min_over_time(ovn_db_cluster_id{db_name="OVN_Northbound"}[5m])) by (cluster_id, namespace)) by (namespace) > 1
        for: 5m
        labels:
          severity: critical
      - alert: OVNKubernetesSouthboundDatabaseClusterIDError
        annotations:
          description: More than one OVN southbound database cluster ID indicates degraded
            OVN database high availability and possible database split brain.
          summary: Multiple OVN southbound database cluster IDs exist.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(count(min_over_time(ovn_db_cluster_id{db_name="OVN_Southbound"}[5m])) by (cluster_id, namespace)) by (namespace) > 1
        for: 5m
        labels:
          severity: critical
      - alert: OVNKubernetesNorthboundDatabaseTermLag
        annotations:
          description: OVN northbound database(s) RAFT term have not been equal which
            may indicate degraded OVN database high availability.
          summary: OVN northbound databases RAFT term have not been equal for a period
            of time.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max(max_over_time(ovn_db_cluster_term{db_name="OVN_Northbound"}[5m])) by (namespace) - min(max_over_time(ovn_db_cluster_term{db_name="OVN_Northbound"}[5m])) by (namespace) > 0
        for: 25m
        labels:
          severity: warning
      - alert: OVNKubernetesSouthboundDatabaseTermLag
        annotations:
          description: OVN southbound database(s) RAFT term have not been equal which
            may indicate degraded OVN database high availability.
          summary: OVN southbound databases RAFT term have not been equal for a period
            of time.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max(max_over_time(ovn_db_cluster_term{db_name="OVN_Southbound"}[5m])) by (namespace) - min(max_over_time(ovn_db_cluster_term{db_name="OVN_Southbound"}[5m])) by (namespace) > 0
        for: 25m
        labels:
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseLeaderError
        annotations:
          description: OVN northbound database(s) have no RAFT leader. Networking control
            plane is degraded.
          summary: OVN northbound database(s) have no RAFT leader
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(max_over_time(ovn_db_cluster_server_role{db_name="OVN_Northbound", server_role="leader"}[5m])) by (namespace) == 0
        for: 5m
        labels:
          severity: critical
      - alert: OVNKubernetesSouthboundDatabaseLeaderError
        annotations:
          description: OVN southbound database(s) have no leader. Networking control plane
            is degraded.
          summary: OVN southbound database(s) have no RAFT leader
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(max_over_time(ovn_db_cluster_server_role{db_name="OVN_Southbound", server_role="leader"}[5m])) by (namespace) == 0
        for: 5m
        labels:
          severity: critical
      - alert: OVNKubernetesNorthboundDatabaseMultipleLeadersError
        annotations:
          description: OVN northbound database(s) have multiple RAFT leaders which may
            indicate degraded OVN database high availability.
          summary: OVN northbound database(s) have multiple RAFT leaders
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(min_over_time(ovn_db_cluster_server_role{db_name="OVN_Northbound", server_role="leader"}[1m])) by (leader, namespace) > 1
        for: 5m
        labels:
          severity: critical
      - alert: OVNKubernetesSouthboundDatabaseMultipleLeadersError
        annotations:
          description: OVN southbound database(s) have multiple RAFT leaders which may
            indicate degraded OVN database high availability.
          summary: OVN southbound database(s) have multiple RAFT leaders
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(min_over_time(ovn_db_cluster_server_role{db_name="OVN_Southbound", server_role="leader"}[1m])) by (leader, namespace) > 1
        for: 5m
        labels:
          severity: critical
      - alert: OVNKubernetesNorthboundDatabaseClusterMemberError
        annotations:
          description: OVN northbound database server(s) has not been a RAFT cluster member
            for a period of time which may indicate degraded OVN database high availability
            cluster.
          summary: OVN northbound database server(s) has not been a member of the databases
            high availability for a period of time.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          min_over_time(cluster:ovn_db_nbdb_not_cluster_member:abs[5m]) != 0
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: warning
      - alert: OVNKubernetesSouthboundDatabaseClusterMemberError
        annotations:
          description: OVN southbound database server(s) has not been a RAFT cluster member
            for a period of time which may indicate degraded OVN database high availability.
          summary: OVN southbound database server(s) has not been a member of the databases
            high availability for a period of time.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          min_over_time(cluster:ovn_db_sbdb_not_cluster_member:abs[5m]) != 0
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseInboundConnectionError
        annotations:
          description: OVN northbound database server(s) is experiencing inbound RAFT
            connectivity errors which may indicate degraded OVN database high availability.
          summary: OVN northbound database server(s) is experiencing inbound RAFT connectivity
            errors.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          # ..error_total is set to zero when error resolves itself
          min_over_time(ovn_db_cluster_inbound_connections_error_total{db_name="OVN_Northbound"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
      - alert: OVNKubernetesSouthboundDatabaseInboundConnectionError
        annotations:
          description: OVN southbound database server(s) is experiencing inbound RAFT
            connectivity errors which may indicate degraded OVN database high availability.
          summary: OVN southbound database server(s) is experiencing inbound RAFT connectivity
            errors.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          # ..error_total is set to zero when error resolves itself
          min_over_time(ovn_db_cluster_inbound_connections_error_total{db_name="OVN_Southbound"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseOutboundConnectionError
        annotations:
          description: OVN northbound database server(s) outbound RAFT connectivity errors
            may indicate degraded OVN database high availability.
          summary: OVN northbound database server(s) is experiencing outbound RAFT connectivity
            errors.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          # ..error_total is set to zero when error resolves itself
          min_over_time(ovn_db_cluster_outbound_connections_error_total{db_name="OVN_Northbound"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
      - alert: OVNKubernetesSouthboundDatabaseOutboundConnectionError
        annotations:
          description: OVN southbound database server(s) outbound RAFT connectivity errors
            which may indicate degraded OVN database high availability.
          summary: OVN southbound database server(s) is experiencing outbound RAFT connectivity
            errors.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          # ..error_total is set to zero when error resolves itself
          min_over_time(ovn_db_cluster_outbound_connections_error_total{db_name="OVN_Southbound"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseInboundConnectionMissing
        annotations:
          description: OVN northbound database server(s) do not have expected number of
            inbound connections for a RAFT cluster which may indicate degraded OVN database
            high availability.
          summary: OVN northbound database server(s) do not have expected number of inbound
            RAFT connections.
        expr: |
          # Expected sum of inbound connections is number of control plane nodes * number of control plane nodes minus one
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          min_over_time(cluster:ovn_db_nbdb_missing_inbound_connections:abs[5m]) != 0
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: warning
      - alert: OVNKubernetesSouthboundDatabaseInboundConnectionMissing
        annotations:
          description: OVN southbound database server(s) do not have expected number of
            inbound connections for a RAFT cluster which may indicate degraded OVN database
            high availability.
          summary: OVN southbound database server(s) do not have expected number of inbound
            RAFT connections.
        expr: |
          # Expected sum of inbound connections is number of control plane nodes * number of control plane nodes minus one
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          min_over_time(cluster:ovn_db_sbdb_missing_inbound_connections:abs[5m]) != 0
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseOutboundConnectionMissing
        annotations:
          description: OVN northbound database server(s) do not have expected number of
            outbound connections for a RAFT cluster which may indicate degraded OVN database
            high availability.
          summary: OVN northbound database server(s) do not have expected number of outbound
            RAFT connections.
        expr: |
          # Expected sum of outbound connections is number of control plane nodes * number of control plane nodes minus one
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          min_over_time(cluster:ovn_db_nbdb_missing_outbound_connections:abs[5m]) != 0
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: warning
      - alert: OVNKubernetesSouthboundDatabaseOutboundConnectionMissing
        annotations:
          description: OVN southbound database server(s) do not have expected number of
            outbound connections for a RAFT cluster which may indicate degraded OVN database
            high availability.
          summary: OVN southbound database server(s) do not have expected number of outbound
            RAFT connections.
        expr: |
          # Expected sum of outbound connections is number of control plane nodes * number of control plane nodes minus one
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          min_over_time(cluster:ovn_db_sbdb_missing_outbound_connections:abs[5m]) != 0
        for: 5m
        labels:
          namespace: openshift-ovn-kubernetes
          severity: warning
      - alert: OVNKubernetesNorthboundDatabaseCPUUsageHigh
        annotations:
          description: High OVN northbound CPU usage indicates high load on the networking
            control plane.
          summary: OVN northbound database {{ $labels.instance }} is greater than {{ $value
            | humanizePercentage }} percent CPU usage for a period of time.
        expr: (sum(rate(container_cpu_usage_seconds_total{container="nbdb"}[5m])) BY (instance,
          name, namespace)) > 0.8
        for: 15m
        labels:
          severity: info
      - alert: OVNKubernetesSouthboundDatabaseCPUUsageHigh
        annotations:
          description: High OVN southbound CPU usage indicates high load on the networking
            control plane.
          summary: OVN southbound database {{ $labels.instance }} is greater than {{ $value
            | humanizePercentage }} percent CPU usage for a period of time.
        expr: (sum(rate(container_cpu_usage_seconds_total{container="sbdb"}[5m])) BY (instance,
          name, namespace)) > 0.8
        for: 15m
        labels:
          severity: info
      - alert: OVNKubernetesNorthdInactive
        annotations:
          description: Exactly one OVN northd must have an active status within the high
            availability set. Networking control plane is degraded.
          summary: Exactly one OVN northd instance must have an active status.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          count(ovn_northd_status == 1) by (namespace) != 1
        for: 5m
        labels:
          severity: critical
  openshift-ovn-kubernetes-networking-rules-e8745f2f-b2b6-4fa3-b12c-1def6fa99d0d.yaml: |
    groups:
    - name: cluster-network-operator-ovn.rules
      rules:
      - alert: NodeWithoutOVNKubeNodePodRunning
        annotations:
          description: |
            Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the
            node may continue to have connectivity but any changes to the networking control plane will not be implemented.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NodeWithoutOVNKubeNodePodRunning.md
          summary: All Linux nodes should be running an ovnkube-node pod, {{ $labels.node
            }} is not.
        expr: |
          (kube_node_info unless on(node) (kube_pod_info{namespace="openshift-ovn-kubernetes",pod=~"ovnkube-node.*"}
          or kube_node_labels{label_kubernetes_io_os="windows"})) > 0
        for: 20m
        labels:
          severity: warning
      - alert: OVNKubernetesControllerDisconnectedSouthboundDatabase
        annotations:
          description: |
            Networking is degraded on nodes when OVN controller is not connected to OVN southbound database connection. No networking control plane updates will be applied to the node.
          summary: Networking control plane is degraded on node {{ $labels.node }} because
            OVN controller is not connected to OVN southbound database.
        expr: |
          max_over_time(ovn_controller_southbound_database_connected[5m]) == 0
        for: 10m
        labels:
          severity: warning
      - alert: OVNKubernetesNodePodAddError
        annotations:
          description: OVN Kubernetes experiences pod creation errors at an elevated rate.
            The pods will be retried.
          summary: OVN Kubernetes is experiencing pod creation errors at an elevated rate.
        expr: |
          (sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="ADD",err="true"}[5m]))
            /
          sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="ADD"}[5m])))
          > 0.1
        for: 15m
        labels:
          severity: warning
      - alert: OVNKubernetesNodePodDeleteError
        annotations:
          description: OVN Kubernetes experiences pod deletion errors at an elevated rate.
            The pods will be retried.
          summary: OVN Kubernetes experiencing pod deletion errors at an elevated rate.
        expr: |
          (sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="DEL",err="true"}[5m]))
            /
          sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="DEL"}[5m])))
          > 0.1
        for: 15m
        labels:
          severity: warning
      - alert: OVNKubernetesResourceRetryFailure
        annotations:
          description: |
            OVN Kubernetes failed to apply networking control plane configuration after several attempts. This might be because the configuration
            provided by the user is invalid or because of an internal error. As a consequence, the cluster might have a degraded status.
          summary: OVN Kubernetes failed to apply networking control plane configuration.
        expr: increase(ovnkube_resource_retry_failures_total[10m]) > 0
        labels:
          severity: warning
kind: ConfigMap
metadata:
  creationTimestamp: "2023-03-09T14:33:01Z"
  labels:
    managed-by: prometheus-operator
    prometheus-name: k8s
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:openshift-cloud-credential-operator-cloud-credential-operator-alerts-8bfd7282-36ce-4525-8b44-8dff286ebc01.yaml: {}
        f:openshift-cluster-machine-approver-machineapprover-rules-c32204e6-8c48-458e-ac3a-eccfadee0566.yaml: {}
        f:openshift-cluster-node-tuning-operator-node-tuning-operator-d2d3b625-644c-47f8-b778-6d719743f28a.yaml: {}
        f:openshift-cluster-samples-operator-samples-operator-alerts-f654ce69-ca3a-4d68-833a-2925ec3688ca.yaml: {}
        f:openshift-cluster-storage-operator-prometheus-0bca8690-4373-405d-bc81-b874e64c76e9.yaml: {}
        f:openshift-cluster-version-cluster-version-operator-d6ae0489-62c1-4fee-b48f-91d1b937017b.yaml: {}
        f:openshift-dns-operator-dns-d1915c24-0808-41bf-a3c3-34c1ec5f3bb3.yaml: {}
        f:openshift-etcd-operator-etcd-prometheus-rules-b5fadeaa-2fb9-44c3-a45f-544ca4464c86.yaml: {}
        f:openshift-image-registry-image-registry-rules-5b09a18c-3fb4-4371-889c-4504ff6ad1fd.yaml: {}
        f:openshift-image-registry-imagestreams-rules-090cce58-af50-40b4-b2b3-bb81c8d4d65d.yaml: {}
        f:openshift-ingress-operator-ingress-operator-29c263a6-62d8-47f9-9ba3-ebcb6fd7b240.yaml: {}
        f:openshift-insights-insights-prometheus-rules-589fecec-3db0-40b5-bddd-8b99cd7ee43d.yaml: {}
        f:openshift-kube-apiserver-api-usage-b27df59b-68ea-4552-accd-fd206e6b9543.yaml: {}
        f:openshift-kube-apiserver-audit-errors-870f666d-bfce-4ed2-81bd-c508887d2478.yaml: {}
        f:openshift-kube-apiserver-cpu-utilization-4f8ebedc-3a83-4194-878e-d72f422f337f.yaml: {}
        f:openshift-kube-apiserver-kube-apiserver-requests-ae382c5b-05c1-4ef5-85a6-7e1b8e10361b.yaml: {}
        f:openshift-kube-apiserver-kube-apiserver-slos-basic-c2e5e4c9-bfac-48c9-9623-b289ac4931a3.yaml: {}
        f:openshift-kube-apiserver-operator-kube-apiserver-operator-a0ceb7db-1588-49c5-b7ef-fab47a36997d.yaml: {}
        f:openshift-kube-apiserver-podsecurity-3e9c2784-7f5d-4f68-8ad2-64013f60b605.yaml: {}
        f:openshift-kube-controller-manager-operator-kube-controller-manager-operator-eb63ef6b-5bd4-4fce-aabf-1a92eebe9141.yaml: {}
        f:openshift-kube-scheduler-operator-kube-scheduler-operator-f8105fa2-5cb4-491b-ac57-42b39164f6c1.yaml: {}
        f:openshift-machine-api-machine-api-operator-prometheus-rules-9acfe89f-501c-468b-b8dd-f2bec38999ad.yaml: {}
        f:openshift-machine-config-operator-machine-config-controller-75640bcc-d020-4e69-8e95-8f442b6ac00b.yaml: {}
        f:openshift-machine-config-operator-machine-config-daemon-de86511a-1cca-4a40-b83c-742dc4b71234.yaml: {}
        f:openshift-marketplace-marketplace-alert-rules-634b3e72-8dca-4164-a8e9-3d9444eaba00.yaml: {}
        f:openshift-monitoring-alertmanager-main-rules-79310b75-0593-4363-a06e-0753c8372101.yaml: {}
        f:openshift-monitoring-cluster-monitoring-operator-prometheus-rules-e392e114-8ec3-49da-a207-8120a57fb217.yaml: {}
        f:openshift-monitoring-kube-state-metrics-rules-2e5136d1-983b-4ab9-be62-c452eadd2b98.yaml: {}
        f:openshift-monitoring-kubernetes-monitoring-rules-35469c8a-01ac-4fee-be62-a98cd5075e32.yaml: {}
        f:openshift-monitoring-node-exporter-rules-63cedfa4-184a-49c6-a563-aa493cecd8f2.yaml: {}
        f:openshift-monitoring-prometheus-k8s-prometheus-rules-f3896e7a-06f0-4524-b501-13e4bf6673eb.yaml: {}
        f:openshift-monitoring-prometheus-k8s-thanos-sidecar-rules-88a5d2b5-2c83-4cdb-817f-c756d68bdc6e.yaml: {}
        f:openshift-monitoring-prometheus-operator-rules-d6eb23ae-a271-48c9-ace5-ca9daa2be1c0.yaml: {}
        f:openshift-monitoring-telemetry-611ae6c7-3fe7-4c1c-ad21-654fbe81578b.yaml: {}
        f:openshift-multus-prometheus-k8s-rules-7b3c0e43-891b-49e9-8138-ab95706596ed.yaml: {}
        f:openshift-operator-lifecycle-manager-olm-alert-rules-89a6a32e-7e2e-45f4-bcd7-ef677783cb6a.yaml: {}
        f:openshift-ovn-kubernetes-master-rules-8bcc512d-2364-455d-895d-b47540f89a48.yaml: {}
        f:openshift-ovn-kubernetes-networking-rules-e8745f2f-b2b6-4fa3-b12c-1def6fa99d0d.yaml: {}
      f:metadata:
        f:labels:
          .: {}
          f:managed-by: {}
          f:prometheus-name: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"7a33d1cb-c2e6-4559-b78c-c50369df7494"}: {}
    manager: PrometheusOperator
    operation: Update
    time: "2023-03-09T14:33:01Z"
  name: prometheus-k8s-rulefiles-0
  namespace: openshift-monitoring
  ownerReferences:
  - apiVersion: monitoring.coreos.com/v1
    blockOwnerDeletion: true
    controller: true
    kind: Prometheus
    name: k8s
    uid: 7a33d1cb-c2e6-4559-b78c-c50369df7494
  resourceVersion: "16203"
  uid: 7fef5a5a-20a9-46d1-9ade-698e5aa80b72
