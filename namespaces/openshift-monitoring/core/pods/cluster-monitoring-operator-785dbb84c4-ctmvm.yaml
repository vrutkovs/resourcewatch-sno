apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.128.0.10/23"],"mac_address":"0a:58:0a:80:00:0a","gateway_ips":["10.128.0.1"],"ip_address":"10.128.0.10/23","gateway_ip":"10.128.0.1"}}'
    k8s.v1.cni.cncf.io/network-status: |-
      [{
          "name": "ovn-kubernetes",
          "interface": "eth0",
          "ips": [
              "10.128.0.10"
          ],
          "mac": "0a:58:0a:80:00:0a",
          "default": true,
          "dns": {}
      }]
    openshift.io/scc: restricted-v2
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
  creationTimestamp: "2023-03-08T12:06:32Z"
  generateName: cluster-monitoring-operator-785dbb84c4-
  labels:
    app: cluster-monitoring-operator
    app.kubernetes.io/name: cluster-monitoring-operator
    pod-template-hash: 785dbb84c4
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:target.workload.openshift.io/management: {}
        f:generateName: {}
        f:labels:
          .: {}
          f:app: {}
          f:app.kubernetes.io/name: {}
          f:pod-template-hash: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"6a588ae4-6da1-49bc-ac6d-c3c99e377413"}: {}
      f:spec:
        f:containers:
          k:{"name":"cluster-monitoring-operator"}:
            .: {}
            f:args: {}
            f:env:
              .: {}
              k:{"name":"POD_NAME"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
              k:{"name":"RELEASE_VERSION"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":8443,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:name: {}
                f:protocol: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/etc/cluster-monitoring-operator/telemetry"}:
                .: {}
                f:mountPath: {}
                f:name: {}
                f:readOnly: {}
              k:{"mountPath":"/etc/tls/private"}:
                .: {}
                f:mountPath: {}
                f:name: {}
                f:readOnly: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:nodeSelector: {}
        f:priorityClassName: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:serviceAccount: {}
        f:serviceAccountName: {}
        f:terminationGracePeriodSeconds: {}
        f:tolerations: {}
        f:volumes:
          .: {}
          k:{"name":"cluster-monitoring-operator-tls"}:
            .: {}
            f:name: {}
            f:secret:
              .: {}
              f:defaultMode: {}
              f:secretName: {}
          k:{"name":"telemetry-config"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:name: {}
            f:name: {}
    manager: kube-controller-manager
    operation: Update
    time: "2023-03-08T12:06:32Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          .: {}
          k:{"type":"PodScheduled"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:message: {}
            f:reason: {}
            f:status: {}
            f:type: {}
    manager: kube-scheduler
    operation: Update
    subresource: status
    time: "2023-03-08T12:06:32Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          f:k8s.ovn.org/pod-networks: {}
    manager: vrutkovs-sno
    operation: Update
    time: "2023-03-08T12:08:24Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          f:k8s.v1.cni.cncf.io/network-status: {}
    manager: multus
    operation: Update
    subresource: status
    time: "2023-03-08T12:19:39Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.128.0.10"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    subresource: status
    time: "2023-03-08T12:19:48Z"
  name: cluster-monitoring-operator-785dbb84c4-ctmvm
  namespace: openshift-monitoring
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: cluster-monitoring-operator-785dbb84c4
    uid: 6a588ae4-6da1-49bc-ac6d-c3c99e377413
  resourceVersion: "11598"
  uid: e47b400c-ff39-4c40-a831-56ed16fb4165
spec:
  containers:
  - args:
    - -namespace=openshift-monitoring
    - -namespace-user-workload=openshift-user-workload-monitoring
    - -configmap=cluster-monitoring-config
    - -release-version=$(RELEASE_VERSION)
    - -logtostderr=true
    - -v=2
    - -cert-file=/etc/tls/private/tls.crt
    - -key-file=/etc/tls/private/tls.key
    - -images=prometheus-operator=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6fbfc2bc5e88c1873b0f349f06c53a53d3e6af61740c92b1c6034eec795aa4b3
    - -images=prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3c97acac3647a99a40d57710a0ec697458c400421730e1a676111beda7caf958
    - -images=prometheus-operator-admission-webhook=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9950a94aa6431101775e7494a808290262a893641b8ce4038a388dcff8d6e5b5
    - -images=configmap-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4b4ca39a93e62682862c5bf7d8846476b001e481a22c0745654c69733e86d243
    - -images=prometheus=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b10ab06cd40456afe5f928c2642832ea92a8ad05b0636b9286b82d39a66eb2d4
    - -images=alertmanager=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aaa094ab3806e1688b381459841ace8bf528b4965f8791cb0353546a518ba543
    - -images=oauth-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5925f28a7682ef5cb91d4a3e3c9f6877ccc339ddb3533ad6376771314a79cb5a
    - -images=node-exporter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3576c4bdb825087fc187266bfc13ac84c644ce18779fe864adc03303fab67063
    - -images=kube-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:16c13ffaa8a7730a23ee5cc3eb33a6f68c03e7b91094e268fc364ec79f79fbf3
    - -images=openshift-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5187f93fb888d3ea851c029034ad5deec7a3e259a7594e055a037fb1cf1bc5b5
    - -images=kube-rbac-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:eef3d1894656818ad393df61d3713115dce777e113b781c1d01bc285ee56ca2c
    - -images=telemeter-client=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3913aff97167c9c482eedb6b3576595856f399550ab69efeb2d2287a5640bbdd
    - -images=prom-label-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9a2f06206b2a3ff6fdd4d4d3a9b4b6ad43ec595fa8e765c47f5565633ba27ecf
    - -images=k8s-prometheus-adapter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fcb29b9f062c78c73e0ec75c28e165465f12875d73fd205223a0948d87a5dfb8
    - -images=thanos=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b7eba12d8e64a5b5fca58ed447fd008f09bd83bcbab7421cf54128a700a20b86
    env:
    - name: RELEASE_VERSION
      value: 4.13.0-ec.4
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:641a2fd2bc9881397dc0c7752096a213892002be44a9f9b9e0fc28848ec85442
    imagePullPolicy: IfNotPresent
    name: cluster-monitoring-operator
    ports:
    - containerPort: 8443
      name: https
      protocol: TCP
    resources:
      requests:
        cpu: 10m
        memory: 75Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000440000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: cluster-monitoring-operator-tls
      readOnly: true
    - mountPath: /etc/cluster-monitoring-operator/telemetry
      name: telemetry-config
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-lw57j
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: vrutkovs-sno
  nodeSelector:
    kubernetes.io/os: linux
    node-role.kubernetes.io/master: ""
  preemptionPolicy: PreemptLowerPriority
  priority: 2000000000
  priorityClassName: system-cluster-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 1000440000
    seLinuxOptions:
      level: s0:c21,c10
    seccompProfile:
      type: RuntimeDefault
  serviceAccount: cluster-monitoring-operator
  serviceAccountName: cluster-monitoring-operator
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 120
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 120
  volumes:
  - configMap:
      defaultMode: 420
      name: telemetry-config
    name: telemetry-config
  - name: cluster-monitoring-operator-tls
    secret:
      defaultMode: 420
      secretName: cluster-monitoring-operator-tls
  - name: kube-api-access-lw57j
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:08:24Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:19:48Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:19:48Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-03-08T12:08:24Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://7d30c85ddc9871d9c4be2a391d12d0a75c02070a07314847643242ba7cfe922a
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:641a2fd2bc9881397dc0c7752096a213892002be44a9f9b9e0fc28848ec85442
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:641a2fd2bc9881397dc0c7752096a213892002be44a9f9b9e0fc28848ec85442
    lastState: {}
    name: cluster-monitoring-operator
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-03-08T12:19:47Z"
  hostIP: 10.0.130.68
  phase: Running
  podIP: 10.128.0.10
  podIPs:
  - ip: 10.128.0.10
  qosClass: Burstable
  startTime: "2023-03-08T12:08:24Z"
