apiVersion: config.openshift.io/v1
kind: ClusterOperator
metadata:
  annotations:
    exclude.release.openshift.io/internal-openshift-hosted: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
  creationTimestamp: "2023-03-09T14:14:35Z"
  generation: 1
  managedFields:
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:exclude.release.openshift.io/internal-openshift-hosted: {}
          f:include.release.openshift.io/self-managed-high-availability: {}
          f:include.release.openshift.io/single-node-developer: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"b7590f0c-c37f-44e0-80d3-7b5df1e0e187"}: {}
      f:spec: {}
    manager: cluster-version-operator
    operation: Update
    time: "2023-03-09T14:14:35Z"
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        .: {}
        f:relatedObjects: {}
    manager: cluster-version-operator
    operation: Update
    subresource: status
    time: "2023-03-09T14:14:35Z"
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions: {}
        f:extension:
          .: {}
          f:lastSyncError: {}
          f:master: {}
          f:worker: {}
    manager: machine-config-operator
    operation: Update
    subresource: status
    time: "2023-03-09T14:22:56Z"
  name: machine-config
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    controller: true
    kind: ClusterVersion
    name: version
    uid: b7590f0c-c37f-44e0-80d3-7b5df1e0e187
  resourceVersion: "6058"
  uid: a2677b7f-e3c9-4360-88a8-42c47852f1d4
spec: {}
status:
  conditions:
  - lastTransitionTime: "2023-03-09T14:21:52Z"
    message: Working towards 4.13.0-0.ci.test-2023-03-09-115511-ci-ln-88xnydk-latest
    status: "True"
    type: Progressing
  - lastTransitionTime: "2023-03-09T14:22:32Z"
    message: One or more machine config pools are degraded, please see `oc get mcp`
      for further details and resolve before upgrading
    reason: DegradedPool
    status: "False"
    type: Upgradeable
  extension:
    lastSyncError: 'error pool master is not ready, retrying. Status: (pool degraded:
      true total: 1, ready 0, updated: 0, unavailable: 1)'
    master: 'pool is degraded because nodes fail with "1 nodes are reporting degraded
      status on sync": "Node vrutkovs-sno is reporting: \"machineconfig.machineconfiguration.openshift.io
      \\\"rendered-master-b84983722a8f041fcce35c8e28efcb54\\\" not found\""'
    worker: all 0 nodes are at latest configuration rendered-worker-b30c18907c1b1658d47b3a459ef99f7d
  relatedObjects:
  - group: ""
    name: openshift-machine-config-operator
    resource: namespaces
  - group: machineconfiguration.openshift.io
    name: ""
    resource: machineconfigpools
  - group: machineconfiguration.openshift.io
    name: ""
    resource: controllerconfigs
  - group: machineconfiguration.openshift.io
    name: ""
    resource: machineconfigs
  - group: machineconfiguration.openshift.io
    name: ""
    resource: kubeletconfigs
  - group: machineconfiguration.openshift.io
    name: ""
    resource: containerruntimeconfigs
  - group: ""
    name: ""
    resource: nodes
